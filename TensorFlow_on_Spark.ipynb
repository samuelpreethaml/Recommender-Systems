{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow on Spark",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwD_nI8h0C9s",
        "colab_type": "code",
        "outputId": "50fa5ebe-2860-4f8f-980d-188c49daf300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USC1XW4X0Ivx",
        "colab_type": "code",
        "outputId": "f385952e-e90d-4cac-9807-41becc7bf699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install tensorflowonspark\n",
        "!pip install sparkdl\n",
        "!pip install tensorframes\n",
        "!pip install kafka\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q  https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xvf spark-2.3.4-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=7dcf485d0269434aa6173f1fc92bbd857004ef63607018a20405f6b2f4ae7e70\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting findspark\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-1.3.0\n",
            "Collecting tensorflowonspark\n",
            "  Downloading https://files.pythonhosted.org/packages/81/c7/bdf9555848905ae6b66477a28691859e65ce93cf9a7dd3cf669c1675088f/tensorflowonspark-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: tensorflowonspark\n",
            "Successfully installed tensorflowonspark-2.0.0\n",
            "Collecting sparkdl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/e6/c520f801b945f3d03dbf47e1abb7a454cda328d1592f9854dcec69bed097/sparkdl-0.2.2-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sparkdl\n",
            "Successfully installed sparkdl-0.2.2\n",
            "Collecting tensorframes\n",
            "  Downloading https://files.pythonhosted.org/packages/59/ae/e8607d0bc5d722694250dcfce59cc9d530e93406079c7d1bf4cb4bbe9d9a/tensorframes-0.2.9-py3-none-any.whl\n",
            "Installing collected packages: tensorframes\n",
            "Successfully installed tensorframes-0.2.9\n",
            "Collecting kafka\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/71/73286e748ac5045b6a669c2fe44b03ac4c5d3d2af9291c4c6fc76438a9a9/kafka-1.3.5-py2.py3-none-any.whl (207kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 6.5MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka\n",
            "Successfully installed kafka-1.3.5\n",
            "spark-2.3.4-bin-hadoop2.7/\n",
            "spark-2.3.4-bin-hadoop2.7/data/\n",
            "spark-2.3.4-bin-hadoop2.7/data/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/multi-channel/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/multi-channel/BGRA.png\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/multi-channel/grayscale.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/multi-channel/chr30.4.184.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/multi-channel/BGRA_alpha_60.png\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/DP802813.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/DP153539.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/54893.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/not-image.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.3.4-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/graphx/\n",
            "spark-2.3.4-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.3.4-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.3.4-bin-hadoop2.7/examples/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderEstimatorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderEstimatorExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_estimator_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.3.4-bin-hadoop2.7/examples/jars/\n",
            "spark-2.3.4-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/yarn/\n",
            "spark-2.3.4-bin-hadoop2.7/yarn/spark-2.3.4-yarn-shuffle.jar\n",
            "spark-2.3.4-bin-hadoop2.7/LICENSE\n",
            "spark-2.3.4-bin-hadoop2.7/R/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/doc/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/doc/index.html\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.3.4-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.3.4-bin-hadoop2.7/bin/\n",
            "spark-2.3.4-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/run-example\n",
            "spark-2.3.4-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.3.4-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/beeline\n",
            "spark-2.3.4-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.3.4-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.3.4-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.3.4-bin-hadoop2.7/python/\n",
            "spark-2.3.4-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.3.4-bin-hadoop2.7/python/dist/\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.3.4-bin-hadoop2.7/python/setup.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.3.4-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.3.4-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.3.4-bin-hadoop2.7/python/lib/\n",
            "spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.3.4-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip\n",
            "spark-2.3.4-bin-hadoop2.7/python/run-tests\n",
            "spark-2.3.4-bin-hadoop2.7/python/README.md\n",
            "spark-2.3.4-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/test_serializers.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/test_broadcast.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.3.4-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.3.4-bin-hadoop2.7/README.md\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-boto.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.3.4-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.3.4-bin-hadoop2.7/kubernetes/\n",
            "spark-2.3.4-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-2.3.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-2.3.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-2.3.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-2.3.4-bin-hadoop2.7/jars/\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jline-2.12.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-encoding-1.8.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scalap-2.11.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-catalyst_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/py4j-0.10.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-network-common_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/metrics-json-3.1.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/metrics-core-3.1.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/logging-interceptor-3.8.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-hive_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-compiler-3.0.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/httpclient-4.5.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-sql_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-mllib_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-yarn_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-databind-2.6.7.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/lz4-java-1.4.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/netty-all-4.1.17.Final.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/objenesis-2.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-column-1.8.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/univocity-parsers-2.5.9.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/chill_2.11-0.8.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/arrow-memory-0.8.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-jackson-1.8.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/arrow-vector-0.8.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hppc-0.7.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/aircompressor-0.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/xz-1.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/orc-mapreduce-1.4.4-nohive.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/generex-1.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-sketch_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/okhttp-3.8.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-net-2.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-core-2.6.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/kubernetes-model-2.0.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/java-xmlbuilder-1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-mesos_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-core_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/okio-1.13.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/kubernetes-client-3.0.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-hadoop-1.8.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/arrow-format-0.8.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/parquet-common-1.8.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/base64-2.3.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/bcprov-jdk15on-1.58.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/avro-1.7.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-graphx_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jets3t-0.9.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/httpcore-4.4.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-launcher_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-streaming_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-kvstore_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/orc-core-1.4.4-nohive.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-repl_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/janino-3.0.8.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/chill-java-0.8.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/spark-tags_2.11-2.3.4.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.3.4-bin-hadoop2.7/jars/snakeyaml-1.15.jar\n",
            "spark-2.3.4-bin-hadoop2.7/RELEASE\n",
            "spark-2.3.4-bin-hadoop2.7/NOTICE\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.3.4-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.3.4-bin-hadoop2.7/conf/\n",
            "spark-2.3.4-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.3.4-bin-hadoop2.7/conf/spark-env.sh.template\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLmBRBCo0PEJ",
        "colab_type": "code",
        "outputId": "f24d38a0-b0f6-49d0-b64d-dbaf0233d6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "import findspark\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
        "import pyspark.sql.functions as F \n",
        "import sklearn\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import pyspark.mllib\n",
        "import keras\n",
        "import tensorflowonspark as tfos\n",
        "from tensorflowonspark import TFCluster\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorframes as tfs\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TREWsjjy0SpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir /root/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwkjS0aG0mL0",
        "colab_type": "code",
        "outputId": "94eeae42-cf2c-4d6f-c2bb-77e30d1f4de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /root/.kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvz2Ver20nL8",
        "colab_type": "code",
        "outputId": "894cb90e-e6fe-400f-d7b1-32f961d477cd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9eac9bc8-78e0-4911-a3da-1f9c3df80b48\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9eac9bc8-78e0-4911-a3da-1f9c3df80b48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"samuelpreethaml\",\"key\":\"14617aaa25068d81fb4b1232097ec0ec\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykH4kcT0kC3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir /root/recommender"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WrZQK1FkLIu",
        "colab_type": "code",
        "outputId": "aa1f0c2f-d99c-454a-c7de-68b1bb1ba6c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /root/recommender"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/recommender\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RQqaFac609n",
        "colab_type": "code",
        "outputId": "467bb784-4dda-4a6d-efbc-9d929d51522a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!kaggle competitions download -c santander-product-recommendation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /root/recommender\n",
            "\r  0% 0.00/2.28M [00:00<?, ?B/s]\n",
            "\r100% 2.28M/2.28M [00:00<00:00, 75.7MB/s]\n",
            "Downloading test_ver2.csv.zip to /root/recommender\n",
            " 40% 5.00M/12.4M [00:00<00:00, 42.4MB/s]\n",
            "100% 12.4M/12.4M [00:00<00:00, 79.2MB/s]\n",
            "Downloading train_ver2.csv.zip to /root/recommender\n",
            " 98% 209M/214M [00:02<00:00, 79.1MB/s]\n",
            "100% 214M/214M [00:02<00:00, 94.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMMAT0e_8aeP",
        "colab_type": "code",
        "outputId": "ef1eb4da-d04a-4ef5-cc7c-270a17269770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!unzip train_ver2.csv.zip\n",
        "!unzip test_ver2.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train_ver2.csv.zip\n",
            "  inflating: train_ver2.csv          \n",
            "Archive:  test_ver2.csv.zip\n",
            "  inflating: test_ver2.csv           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrPmys6QQFJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages databricks:tensorframes:0.6.0-s_2.11 pyspark-shell'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgWdhwfCzWmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext(master='local',appName='test1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl5b6BQ5SXOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession(sc)\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6rElo3d09H7",
        "colab_type": "code",
        "outputId": "0195dd14-a698-44a6-d38f-e12f909d325d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/yahoo/TensorFlowOnSpark.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlowOnSpark'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/205)\u001b[K\rremote: Counting objects:   1% (3/205)\u001b[K\rremote: Counting objects:   2% (5/205)\u001b[K\rremote: Counting objects:   3% (7/205)\u001b[K\rremote: Counting objects:   4% (9/205)\u001b[K\rremote: Counting objects:   5% (11/205)\u001b[K\rremote: Counting objects:   6% (13/205)\u001b[K\rremote: Counting objects:   7% (15/205)\u001b[K\rremote: Counting objects:   8% (17/205)\u001b[K\rremote: Counting objects:   9% (19/205)\u001b[K\rremote: Counting objects:  10% (21/205)\u001b[K\rremote: Counting objects:  11% (23/205)\u001b[K\rremote: Counting objects:  12% (25/205)\u001b[K\rremote: Counting objects:  13% (27/205)\u001b[K\rremote: Counting objects:  14% (29/205)\u001b[K\rremote: Counting objects:  15% (31/205)\u001b[K\rremote: Counting objects:  16% (33/205)\u001b[K\rremote: Counting objects:  17% (35/205)\u001b[K\rremote: Counting objects:  18% (37/205)\u001b[K\rremote: Counting objects:  19% (39/205)\u001b[K\rremote: Counting objects:  20% (41/205)\u001b[K\rremote: Counting objects:  21% (44/205)\u001b[K\rremote: Counting objects:  22% (46/205)\u001b[K\rremote: Counting objects:  23% (48/205)\u001b[K\rremote: Counting objects:  24% (50/205)\u001b[K\rremote: Counting objects:  25% (52/205)\u001b[K\rremote: Counting objects:  26% (54/205)\u001b[K\rremote: Counting objects:  27% (56/205)\u001b[K\rremote: Counting objects:  28% (58/205)\u001b[K\rremote: Counting objects:  29% (60/205)\u001b[K\rremote: Counting objects:  30% (62/205)\u001b[K\rremote: Counting objects:  31% (64/205)\u001b[K\rremote: Counting objects:  32% (66/205)\u001b[K\rremote: Counting objects:  33% (68/205)\u001b[K\rremote: Counting objects:  34% (70/205)\u001b[K\rremote: Counting objects:  35% (72/205)\u001b[K\rremote: Counting objects:  36% (74/205)\u001b[K\rremote: Counting objects:  37% (76/205)\u001b[K\rremote: Counting objects:  38% (78/205)\u001b[K\rremote: Counting objects:  39% (80/205)\u001b[K\rremote: Counting objects:  40% (82/205)\u001b[K\rremote: Counting objects:  41% (85/205)\u001b[K\rremote: Counting objects:  42% (87/205)\u001b[K\rremote: Counting objects:  43% (89/205)\u001b[K\rremote: Counting objects:  44% (91/205)\u001b[K\rremote: Counting objects:  45% (93/205)\u001b[K\rremote: Counting objects:  46% (95/205)\u001b[K\rremote: Counting objects:  47% (97/205)\u001b[K\rremote: Counting objects:  48% (99/205)\u001b[K\rremote: Counting objects:  49% (101/205)\rremote: Counting objects:  50% (103/205)\u001b[K\rremote: Counting objects:  51% (105/205)\u001b[K\rremote: Counting objects:  52% (107/205)\u001b[K\rremote: Counting objects:  53% (109/205)\u001b[K\rremote: Counting objects:  54% (111/205)\u001b[K\rremote: Counting objects:  55% (113/205)\u001b[K\rremote: Counting objects:  56% (115/205)\u001b[K\rremote: Counting objects:  57% (117/205)\u001b[K\rremote: Counting objects:  58% (119/205)\u001b[K\rremote: Counting objects:  59% (121/205)\u001b[K\rremote: Counting objects:  60% (123/205)\u001b[K\rremote: Counting objects:  61% (126/205)\u001b[K\rremote: Counting objects:  62% (128/205)\u001b[K\rremote: Counting objects:  63% (130/205)\u001b[K\rremote: Counting objects:  64% (132/205)\u001b[K\rremote: Counting objects:  65% (134/205)\u001b[K\rremote: Counting objects:  66% (136/205)\u001b[K\rremote: Counting objects:  67% (138/205)\u001b[K\rremote: Counting objects:  68% (140/205)\u001b[K\rremote: Counting objects:  69% (142/205)\u001b[K\rremote: Counting objects:  70% (144/205)\u001b[K\rremote: Counting objects:  71% (146/205)\u001b[K\rremote: Counting objects:  72% (148/205)\u001b[K\rremote: Counting objects:  73% (150/205)\u001b[K\rremote: Counting objects:  74% (152/205)\u001b[K\rremote: Counting objects:  75% (154/205)\u001b[K\rremote: Counting objects:  76% (156/205)\u001b[K\rremote: Counting objects:  77% (158/205)\u001b[K\rremote: Counting objects:  78% (160/205)\u001b[K\rremote: Counting objects:  79% (162/205)\u001b[K\rremote: Counting objects:  80% (164/205)\u001b[K\rremote: Counting objects:  81% (167/205)\u001b[K\rremote: Counting objects:  82% (169/205)\u001b[K\rremote: Counting objects:  83% (171/205)\u001b[K\rremote: Counting objects:  84% (173/205)\u001b[K\rremote: Counting objects:  85% (175/205)\u001b[K\rremote: Counting objects:  86% (177/205)\u001b[K\rremote: Counting objects:  87% (179/205)\u001b[K\rremote: Counting objects:  88% (181/205)\u001b[K\rremote: Counting objects:  89% (183/205)\u001b[K\rremote: Counting objects:  90% (185/205)\u001b[K\rremote: Counting objects:  91% (187/205)\u001b[K\rremote: Counting objects:  92% (189/205)\u001b[K\rremote: Counting objects:  93% (191/205)\u001b[K\rremote: Counting objects:  94% (193/205)\u001b[K\rremote: Counting objects:  95% (195/205)\u001b[K\rremote: Counting objects:  96% (197/205)\u001b[K\rremote: Counting objects:  97% (199/205)\u001b[K\rremote: Counting objects:  98% (201/205)\u001b[K\rremote: Counting objects:  99% (203/205)\u001b[K\rremote: Counting objects: 100% (205/205)\u001b[K\rremote: Counting objects: 100% (205/205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 3417 (delta 70), reused 158 (delta 48), pack-reused 3212\u001b[K\n",
            "Receiving objects: 100% (3417/3417), 8.49 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (1962/1962), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1i8El8k2kbT",
        "colab_type": "code",
        "outputId": "077de89c-1ecd-4dde-97ec-b1e58fd06df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd TensorFlowOnSpark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/recommender/TensorFlowOnSpark\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_sgFG6C1Gq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export TFoS_HOME=$(pwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Reru-2I71taq",
        "colab_type": "code",
        "outputId": "0f428c0f-3ebd-42ee-fdea-4443db5d6380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 31397, done.\u001b[K\n",
            "remote: Total 31397 (delta 0), reused 0 (delta 0), pack-reused 31397\u001b[K\n",
            "Receiving objects: 100% (31397/31397), 510.80 MiB | 43.90 MiB/s, done.\n",
            "Resolving deltas: 100% (19803/19803), done.\n",
            "Checking out files: 100% (3114/3114), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpFNjUKz1uSI",
        "colab_type": "code",
        "outputId": "58147c35-dfe8-48e8-83b1-365387b8c57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/recommender/TensorFlowOnSpark/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F3i6Nel1wY7",
        "colab_type": "code",
        "outputId": "0806bba3-54b1-42a9-f90c-c6e5d423e638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r official/requirements.txt\n",
        "!export TF_MODELS=$(pwd)\n",
        "!export PYTHONPATH=$PYTHONPATH:$(pwd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 1)) (1.7.11)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 2)) (1.14.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 3)) (1.5.6)\n",
            "Collecting mlperf_compliance==0.0.10 (from -r official/requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/08/f2febd8cbd5c9371f7dab311e90400d83238447ba7609b3bf0145b4cb2a2/mlperf_compliance-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 5)) (1.16.5)\n",
            "Requirement already satisfied: oauth2client>=4.1.2 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 6)) (4.1.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 7)) (0.24.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 8)) (5.4.8)\n",
            "Collecting py-cpuinfo>=3.3.0 (from -r official/requirements.txt (line 9))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 10)) (1.3.1)\n",
            "Collecting typing (from -r official/requirements.txt (line 11))\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/2e/b480ee1b75e6d17d2993738670e75c1feeb9ff7f64452153cf018051cc92/typing-3.7.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 12)) (0.6.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 13)) (0.29.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 14)) (3.0.3)\n",
            "Collecting opencv-python-headless (from -r official/requirements.txt (line 15))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/dc/b250f03ab68068033fd2356428c1357431d8ebc6a26405098e0f27c94f7a/opencv_python_headless-4.1.1.26-cp36-cp36m-manylinux1_x86_64.whl (22.1MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 16)) (3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from -r official/requirements.txt (line 17)) (4.3.0)\n",
            "Obtaining pycocotools from git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI (from -r official/requirements.txt (line 18))\n",
            "  Cloning https://github.com/cocodataset/cocoapi to ./src/pycocotools\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi /root/recommender/TensorFlowOnSpark/models/src/pycocotools\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (0.0.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->-r official/requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->-r official/requirements.txt (line 2)) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->-r official/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r official/requirements.txt (line 3)) (2.5.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->-r official/requirements.txt (line 6)) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->-r official/requirements.txt (line 6)) (0.4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.1.2->-r official/requirements.txt (line 6)) (0.2.6)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->-r official/requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r official/requirements.txt (line 14)) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r official/requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r official/requirements.txt (line 14)) (0.10.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->-r official/requirements.txt (line 17)) (0.46)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->-r official/requirements.txt (line 18)) (41.2.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->-r official/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery>=0.31.0->-r official/requirements.txt (line 2)) (1.14.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->-r official/requirements.txt (line 3)) (1.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->-r official/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->-r official/requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery>=0.31.0->-r official/requirements.txt (line 2)) (1.6.0)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-5.0.0-cp36-none-any.whl size=18685 sha256=e0ddc8fa2fac4200637b399c262d36a203c9342c738f15ad8884ad188901e4f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
            "Successfully built py-cpuinfo\n",
            "Installing collected packages: mlperf-compliance, py-cpuinfo, typing, opencv-python-headless, pycocotools\n",
            "  Found existing installation: pycocotools 2.0.0\n",
            "    Uninstalling pycocotools-2.0.0:\n",
            "      Successfully uninstalled pycocotools-2.0.0\n",
            "  Running setup.py develop for pycocotools\n",
            "Successfully installed mlperf-compliance-0.0.10 opencv-python-headless-4.1.1.26 py-cpuinfo-5.0.0 pycocotools typing-3.7.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqiq2Z60xJg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp official -r '/usr/local/lib/python3.6/dist-packages'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIz-HP4E22KS",
        "colab_type": "code",
        "outputId": "b02c287a-4046-486b-cc7a-932b72649eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/spark/conf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark/conf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzDW-vGYzLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp spark-env.sh.template spark-env.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ellFlzjU99F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"SPARK_WORKER_CORES=3\">>spark-env.sh\n",
        "!echo \"SPARK_WORKER_INSTANCES=3\">>spark-env.sh\n",
        "!echo \"SPARK_WORKER_MEMORY=3G\">>spark-env.sh\n",
        "!echo \"export SPARK_MASTER_HOST='423fe09a2473'\">>spark-env.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5NYab5hbeGg",
        "colab_type": "code",
        "outputId": "775a2b9f-b9a7-4ccf-844f-4ab4a0faba71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!${SPARK_HOME}/.//sbin/start-master.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting org.apache.spark.deploy.master.Master, logging to /content/spark/logs/spark--org.apache.spark.deploy.master.Master-1-423fe09a2473.out\n",
            "failed to launch: nice -n 0 /content/spark/bin/spark-class org.apache.spark.deploy.master.Master --host 423fe09a2473 --port 7077 --webui-port 8080\n",
            "  Spark Command: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -cp /content/spark/conf/:/content/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host 423fe09a2473 --port 7077 --webui-port 8080\n",
            "  ========================================\n",
            "  /content/spark/bin/spark-class: line 96: CMD: bad array subscript\n",
            "full log in /content/spark/logs/spark--org.apache.spark.deploy.master.Master-1-423fe09a2473.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRL1lW6o10iQ",
        "colab_type": "code",
        "outputId": "9969862a-6cd7-4166-9550-a6c071b59b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!${SPARK_HOME}/.//sbin/start-slave.sh spark://423fe09a2473:7077"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting org.apache.spark.deploy.worker.Worker, logging to /content/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-423fe09a2473.out\n",
            "failed to launch: nice -n 0 /content/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://423fe09a2473:7077\n",
            "  Failed to find Spark jars directory (/content/spark/assembly/target/scala-2.12/jars).\n",
            "  You need to build Spark with the target \"package\" before running this program.\n",
            "full log in /content/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-423fe09a2473.out\n",
            "org.apache.spark.deploy.worker.Worker running as process 1935.  Stop it first.\n",
            "org.apache.spark.deploy.worker.Worker running as process 1997.  Stop it first.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BjedxR015gU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOLKqKvaH58",
        "colab_type": "code",
        "outputId": "8c7acf81-6129-472d-a7c9-86234871a125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /usr/local/lib/python3.6/dist-packages/tensorflowonspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflowonspark\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KJ85q9uYok-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm gpu_info.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBmlhwCzaT8A",
        "colab_type": "code",
        "outputId": "1428e73a-c4a5-4103-fe03-0b05f424d91b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-96333aea-f716-4d7a-b99d-513e1a7c6c9d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-96333aea-f716-4d7a-b99d-513e1a7c6c9d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving gpu_info.py to gpu_info.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gpu_info.py': b'# Copyright 2017 Yahoo Inc.\\n# Licensed under the terms of the Apache 2.0 license.\\n# Please see LICENSE file in the project root for terms.\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import nested_scopes\\nfrom __future__ import print_function\\n\\nimport ctypes as ct\\nimport logging\\nimport platform\\nimport random\\nimport subprocess\\nimport time\\n\\nMAX_RETRIES = 3           #: Maximum retries to allocate GPUs\\n\\n\\ndef _get_gpu():\\n  \"\"\"*DEPRECATED*. Allocates first available GPU using cudaSetDevice(), or returns 0 otherwise.\"\"\"\\n  # Note: this code executes, but Tensorflow subsequently complains that the \"current context was not created by the StreamExecutor cuda_driver API\"\\n  system = platform.system()\\n  if system == \"Linux\":\\n    libcudart = ct.cdll.LoadLibrary(\"libcudart.so\")\\n  elif system == \"Darwin\":\\n    libcudart = ct.cdll.LoadLibrary(\"libcudart.dylib\")\\n  elif system == \"Windows\":\\n    libcudart = ct.windll.LoadLibrary(\"libcudart.dll\")\\n  else:\\n    raise NotImplementedError(\"Cannot identify system.\")\\n\\n  device_count = ct.c_int()\\n  libcudart.cudaGetDeviceCount(ct.byref(device_count))\\n  gpu = 0\\n  for i in range(device_count.value):\\n    if (0 == libcudart.cudaSetDevice(i) and 0 == libcudart.cudaFree(0)):\\n      gpu = i\\n      break\\n  return gpu\\n\\n\\ndef get_gpus(num_gpu=1, worker_index=-1):\\n  \"\"\"Get list of free GPUs according to nvidia-smi.\\n\\n  This will retry for ``MAX_RETRIES`` times until the requested number of GPUs are available.\\n\\n  Args:\\n    :num_gpu: number of GPUs desired.\\n    :worker_index: index \"hint\" for allocation of available GPUs.\\n\\n  Returns:\\n    Comma-delimited string of GPU ids, or raises an Exception if the requested number of GPUs could not be found.\\n  \"\"\"\\n  # get list of gpus (index, uuid)\\n  list_gpus = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"]).decode()\\n  logging.debug(\"all GPUs:\\\\n{0}\".format(list_gpus))\\n\\n  # parse index and guid\\n  gpus = [x for x in list_gpus.split(\\'\\\\n\\') if len(x) > 0]\\n\\n  def parse_gpu(gpu_str):\\n    cols = gpu_str.split(\\' \\')\\n    return cols[5].split(\\')\\')[0], cols[1].split(\\':\\')[0]\\n  gpu_list = [parse_gpu(gpu) for gpu in gpus]\\n\\n  free_gpus = []\\n  retries = 0\\n  while len(free_gpus) < num_gpu and retries < MAX_RETRIES:\\n    smi_output = subprocess.check_output([\"nvidia-smi\", \"--format=csv,noheader,nounits\", \"--query-compute-apps=gpu_uuid\"]).decode()\\n    logging.debug(\"busy GPUs:\\\\n{0}\".format(smi_output))\\n    busy_uuids = [x for x in smi_output.split(\\'\\\\n\\') if len(x) > 0]\\n    for uuid, index in gpu_list:\\n      free_gpus.append(index)\\n\\n    if len(free_gpus) < num_gpu:\\n      logging.warn(\"Unable to find available GPUs: requested={0}, available={1}\".format(num_gpu, len(free_gpus)))\\n      retries += 1\\n      time.sleep(30 * retries)\\n      free_gpus = []\\n\\n  logging.info(\"Available GPUs: {}\".format(free_gpus))\\n\\n  # if still can\\'t find available GPUs, raise exception\\n  if len(free_gpus) < num_gpu:\\n    smi_output = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-compute-apps=gpu_uuid,pid,process_name,used_gpu_memory\"]).decode()\\n    logging.info(\": {0}\".format(smi_output))\\n    raise Exception(\"Unable to find {} free GPU(s)\\\\n{}\".format(num_gpu, smi_output))\\n\\n  # Get logical placement\\n  num_available = len(free_gpus)\\n  if worker_index == -1:\\n    # use original random placement\\n    random.shuffle(free_gpus)\\n    proposed_gpus = free_gpus[:num_gpu]\\n  else:\\n    # ordered by worker index\\n    if worker_index * num_gpu + num_gpu > num_available:\\n      worker_index = worker_index * num_gpu % num_available\\n    proposed_gpus = free_gpus[worker_index * num_gpu:(worker_index * num_gpu + num_gpu)]\\n  logging.info(\"Proposed GPUs: {}\".format(proposed_gpus))\\n\\n  return \\',\\'.join(str(x) for x in proposed_gpus)\\n\\n\\n# Function to get the gpu information\\ndef _get_free_gpu(max_gpu_utilization=40, min_free_memory=0.5, num_gpu=1):\\n  \"\"\"Get available GPUs according to utilization thresholds.\\n\\n  Args:\\n    :max_gpu_utilization: percent utilization threshold to consider a GPU \"free\"\\n    :min_free_memory: percent free memory to consider a GPU \"free\"\\n    :num_gpu: number of requested GPUs\\n\\n  Returns:\\n    A tuple of (available_gpus, minimum_free_memory), where available_gpus is a comma-delimited string of GPU ids, and minimum_free_memory\\n    is the lowest amount of free memory available on the available_gpus.\\n\\n  \"\"\"\\n  def get_gpu_info():\\n    # Get the gpu information\\n    gpu_info = subprocess.check_output([\"nvidia-smi\", \"--format=csv,noheader,nounits\", \"--query-gpu=index,memory.total,memory.free,memory.used,utilization.gpu\"]).decode()\\n    gpu_info = gpu_info.split(\\'\\\\n\\')\\n\\n    gpu_info_array = []\\n\\n    # Check each gpu\\n    for line in gpu_info:\\n      if len(line) > 0:\\n        gpu_id, total_memory, free_memory, used_memory, gpu_util = line.split(\\',\\')\\n        gpu_memory_util = float(used_memory) / float(total_memory)\\n        gpu_info_array.append((float(gpu_util), gpu_memory_util, gpu_id))\\n\\n    return(gpu_info_array)\\n\\n  # Read the gpu information multiple times\\n  num_times_to_average = 5\\n  current_array = []\\n  for ind in range(num_times_to_average):\\n    current_array.append(get_gpu_info())\\n    time.sleep(1)\\n\\n  # Get number of gpus\\n  num_gpus = len(current_array[0])\\n\\n  # Average the gpu information\\n  avg_array = [(0, 0, str(x)) for x in range(num_gpus)]\\n  for ind in range(num_times_to_average):\\n    for gpu_ind in range(num_gpus):\\n      avg_array[gpu_ind] = (avg_array[gpu_ind][0] + current_array[ind][gpu_ind][0], avg_array[gpu_ind][1] + current_array[ind][gpu_ind][1], avg_array[gpu_ind][2])\\n\\n  for gpu_ind in range(num_gpus):\\n    avg_array[gpu_ind] = (float(avg_array[gpu_ind][0]) / num_times_to_average, float(avg_array[gpu_ind][1]) / num_times_to_average, avg_array[gpu_ind][2])\\n\\n  avg_array.sort()\\n\\n  gpus_found = 0\\n  gpus_to_use = \"\"\\n  free_memory = 1.0\\n  # Return the least utilized GPUs if it\\'s utilized less than max_gpu_utilization and amount of free memory is at least min_free_memory\\n  # Otherwise, run in cpu only mode\\n  for current_gpu in avg_array:\\n    if current_gpu[0] < max_gpu_utilization and (1 - current_gpu[1]) > min_free_memory:\\n      if gpus_found == 0:\\n        gpus_to_use = current_gpu[2]\\n        free_memory = 1 - current_gpu[1]\\n      else:\\n        gpus_to_use = gpus_to_use + \",\" + current_gpu[2]\\n        free_memory = min(free_memory, 1 - current_gpu[1])\\n\\n      gpus_found = gpus_found + 1\\n\\n    if gpus_found == num_gpu:\\n      break\\n\\n  return gpus_to_use, free_memory\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZrGGLFF2N9l",
        "colab_type": "code",
        "outputId": "689f4555-a3e4-4cbb-83c5-94561536e8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /root/recommender/TensorFlowOnSpark/examples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/recommender/TensorFlowOnSpark/examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1O9FEwDdLix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm train_dataset.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aFW64lEcmvR",
        "colab_type": "code",
        "outputId": "762cb6c4-65e7-4ec6-999a-4c3352ffe86b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6758f87-c88a-497c-b2d1-85a87244da8a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f6758f87-c88a-497c-b2d1-85a87244da8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_dataset.py to train_dataset.py\n",
            "Saving train_main.py to train_main.py\n",
            "Saving wide_deep_run_loop.py to wide_deep_run_loop.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_dataset.py': b'\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport os\\nimport sys\\n\\n# pylint: disable=wrong-import-order\\nfrom absl import app as absl_app\\nfrom absl import flags\\nfrom six.moves import urllib\\nimport tensorflow as tf\\n# pylint: enable=wrong-import-order\\n\\nfrom official.utils.flags import core as flags_core\\n\\n\\n\\n_CSV_COLUMNS = [\\'Partitioned_Month\\',\\'Partitioned_Year\\',\\'sex\\',\\'Age\\',\\'New_customer_Index\\',\\'Customer_seniority\\',\\'relation_at_beginning\\',\\'Foreigner_index\\',\\'Province_code\\',\\'Activity_index\\',\\'Gross_income\\',\\'segmentation\\',\\'Saving_Account\\',\\'Guarantees\\',\\'Current Accounts\\',\\'Derivada Account\\',\\'Payroll Account\\',\\'Junior Account\\',\\'M\\xc3\\xa1s particular Account\\',\\'particular Account\\',\\'particular Plus Account\\',\\'Short-term deposits\\',\\'Medium-term deposits\\',\\'Long-term deposits\\',\\'e-account\\',\\'Funds\\',\\'Mortgage\\',\\'Pensions\\',\\'Loans\\',\\'Taxes\\',\\'Credit Card\\',\\'Securities\\',\\'Home Account\\',\\'Payroll\\',\\'Pensions2\\',\\'Direct Debit\\']\\n\\n_CSV_COLUMN_DEFAULTS = [[0], [0], [0], [0], [0],[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0],[0], [0], [0], [0], [0], [0], [0], [0], [0],[0], [0], [0], [0], [0], [0], [0], [0], [0]]\\n\\n_HASH_BUCKET_SIZE = 1000\\n\\n_NUM_EXAMPLES = {\\n    \\'train\\': 12747308,\\n    \\'validation\\': 900000,\\n}\\n\\n\\n\\n\\n\\ndef build_model_columns():\\n  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\\n  # Continuous variable columns\\n  Age = tf.feature_column.numeric_column(\\'Age\\')\\n  Customer_seniority = tf.feature_column.numeric_column(\\'Customer_seniority\\')\\n  Gross_income = tf.feature_column.numeric_column(\\'Gross_income\\')\\n\\n  Partitioned_Month = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'Partitioned_Month\\', [0,1,2,3,4,5,6,7,8,9,10,11])\\n\\n  Partitioned_Year = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'Partitioned_Year\\', [0,1])\\n\\n  sex = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'sex\\', [0,1])\\n\\n  New_customer_Index = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'New_customer_Index\\', [0,1])\\n\\n  relation_at_beginning = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'relation_at_beginning\\', [0,1])\\n  \\n  Foreigner_index = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'Foreigner_index\\', [0,1])\\t  \\n\\t  \\n  Activity_index = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'Activity_index\\', [0,1,2])\\n\\t  \\n  Province_code = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'Province_code\\', [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53])\\n\\n  segmentation = tf.feature_column.categorical_column_with_vocabulary_list(\\n      \\'segmentation\\', [0,1,2])\\n\\n  # To show an example of hashing:\\n  occupation = tf.feature_column.categorical_column_with_hash_bucket(\\n      \\'occupation\\', hash_bucket_size=_HASH_BUCKET_SIZE)\\n\\n  \\n  # Transformations.\\n  age_buckets = tf.feature_column.bucketized_column(\\n      Age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\\n\\n  # Wide columns and deep columns.\\n  base_columns = [\\n      Partitioned_Month, Partitioned_Year, sex, New_customer_Index, relation_at_beginning,Foreigner_index,Province_code,segmentation,\\n      age_buckets,\\n  ]\\n  crossed_columns = [\\n      tf.feature_column.crossed_column(\\n          [\\'education\\', \\'occupation\\'], hash_bucket_size=_HASH_BUCKET_SIZE),\\n      tf.feature_column.crossed_column(\\n          [age_buckets, \\'education\\', \\'occupation\\'],\\n          hash_bucket_size=_HASH_BUCKET_SIZE),\\n  ]\\n\\n\\n  wide_columns = base_columns\\n\\n  deep_columns = [\\n      Age,\\n      Customer_seniority,\\n      Gross_income,\\n      tf.feature_column.indicator_column(Partitioned_Month),\\n      tf.feature_column.indicator_column(Partitioned_Year),\\n      tf.feature_column.indicator_column(sex),\\n      tf.feature_column.indicator_column(New_customer_Index),\\n\\t  tf.feature_column.indicator_column(relation_at_beginning),\\n\\t  tf.feature_column.indicator_column(Foreigner_index),\\n\\t  tf.feature_column.indicator_column(Province_code),\\n\\t  tf.feature_column.indicator_column(segmentation)]\\n  return wide_columns, deep_columns\\n\\n\\ndef input_fn(data_file, num_epochs, shuffle, batch_size):\\n  \"\"\"Generate an input function for the Estimator.\"\"\"\\n  assert tf.gfile.Exists(data_file), (\\n      \\'%s not found. Please make sure you have run train_dataset.py and \\'\\n      \\'set the --data_dir argument to the correct path.\\' % data_file)\\n\\n  def parse_csv(value):\\n    tf.logging.info(\\'Parsing {}\\'.format(data_file))\\n    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\\n    features = dict(zip(_CSV_COLUMNS, columns))\\n    labels = features.pop(\\'Saving_Account\\')\\n    classes = tf.equal(labels,0)  # binary classification\\n    return features, classes\\n\\n  # Extract lines from input files using the Dataset API.\\n  dataset = tf.data.TextLineDataset(data_file)\\n\\n  if shuffle:\\n    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES[\\'train\\'])\\n\\n  dataset = dataset.map(parse_csv, num_parallel_calls=5)\\n\\n  # We call repeat after shuffling, rather than before, to prevent separate\\n  # epochs from blending together.\\n  dataset = dataset.repeat(num_epochs)\\n  dataset = dataset.batch(batch_size)\\n  return dataset\\n\\n\\ndef define_data_download_flags():\\n  \"\"\"Add flags specifying data download arguments.\"\"\"\\n  flags.DEFINE_string(\\n      name=\"data_dir\", default=\"/root/recommender/train.csv\",\\n      help=flags_core.help_wrap(\\n          \"Directory to download and extract data.\"))\\n\\n\\n',\n",
              " 'train_main.py': b'\\n\\nimport os\\n\\n# from absl import app as absl_app\\nfrom absl import flags\\nimport tensorflow as tf\\n\\nfrom official.utils.flags import core as flags_core\\nfrom official.utils.logs import logger\\nfrom official.utils.logs import hooks_helper\\nfrom official.utils.misc import model_helpers\\nimport train_dataset\\nimport wide_deep_run_loop\\n\\n\\nLOSS_PREFIX = {\\'wide\\': \\'linear/\\', \\'deep\\': \\'dnn/\\'}\\n\\n\\ndef define_train_flags():\\n  wide_deep_run_loop.define_wide_deep_flags()\\n  flags.adopt_module_key_flags(wide_deep_run_loop)\\n  flags_core.set_defaults(data_dir=\\'/root/recommender\\',\\n                          model_dir=\\'/tmp/train_model\\',\\n                          train_epochs=2,\\n                          epochs_between_evals=2,\\n                          inter_op_parallelism_threads=0,\\n                          intra_op_parallelism_threads=0,\\n                          batch_size=1000)\\n\\n\\ndef build_estimator(model_dir, model_type, model_column_fn, inter_op, intra_op, ctx):\\n  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\\n  wide_columns, deep_columns = model_column_fn()\\n  hidden_units = [100, 75, 50, 25]\\n\\n  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\\n  # trains faster than GPU for this model.\\n  # Note: adding device_filter to fix: https://github.com/tensorflow/tensorflow/issues/21745\\n  run_config = tf.estimator.RunConfig().replace(\\n      session_config=tf.ConfigProto(device_count={\\'GPU\\': 0},\\n                                    device_filters=[\\'/job:ps\\', \\'/job:%s/task:%d\\' % (ctx.job_name, ctx.task_index)],\\n                                    inter_op_parallelism_threads=inter_op,\\n                                    intra_op_parallelism_threads=intra_op))\\n\\n  if model_type == \\'wide\\':\\n    return tf.estimator.LinearClassifier(\\n        model_dir=model_dir,\\n        feature_columns=wide_columns,\\n        config=run_config)\\n  elif model_type == \\'deep\\':\\n    return tf.estimator.DNNClassifier(\\n        model_dir=model_dir,\\n        feature_columns=deep_columns,\\n        hidden_units=hidden_units,\\n        config=run_config)\\n  else:\\n    return tf.estimator.DNNLinearCombinedClassifier(\\n        model_dir=model_dir,\\n        linear_feature_columns=wide_columns,\\n        dnn_feature_columns=deep_columns,\\n        dnn_hidden_units=hidden_units,\\n        config=run_config)\\n\\n\\ndef run_train(flags_obj, ctx):\\n  \"\"\"Construct all necessary functions and call run_loop.\\n\\n  Args:\\n    flags_obj: Object containing user specified flags.\\n  \"\"\"\\n  train_file = \\'/root/recommender/train.csv\\'\\n  test_file = \\'/root/recommender/test.csv\\'\\n\\n  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\\n  def train_input_fn():\\n    return train_dataset.input_fn(\\n        train_file, flags_obj.epochs_between_evals, True, flags_obj.batch_size)\\n\\n  def eval_input_fn():\\n    return train_dataset.input_fn(test_file, 1, False, flags_obj.batch_size)\\n\\n  tensors_to_log = {\\n      \\'average_loss\\': \\'{loss_prefix}head/truediv\\',\\n      \\'loss\\': \\'{loss_prefix}head/weighted_loss/Sum\\'\\n  }\\n\\n  # Removing run_loop, since we can only invoke train_and_evaluate once\\n  model_helpers.apply_clean(flags.FLAGS)\\n  model = build_estimator(\\n      model_dir=flags_obj.model_dir, model_type=flags_obj.model_type,\\n      model_column_fn=train_dataset.build_model_columns,\\n      inter_op=flags_obj.inter_op_parallelism_threads,\\n      intra_op=flags_obj.intra_op_parallelism_threads,\\n      ctx=ctx)\\n\\n  loss_prefix = LOSS_PREFIX.get(flags_obj.model_type, \\'\\')\\n  tensors_to_log = {k: v.format(loss_prefix=loss_prefix)\\n                    for k, v in tensors_to_log.items()}\\n  logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=10)\\n\\n\\n  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, hooks=[logging_hook])\\n  eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\\n  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\\n\\n\\ndef main_fun(argv, ctx):\\n  sys.argv = argv\\n  define_train_flags()\\n  flags.FLAGS(sys.argv)\\n  tf.logging.set_verbosity(tf.logging.INFO)\\n\\n  with logger.benchmark_context(flags.FLAGS):\\n    run_train(flags.FLAGS, ctx)\\n\\n\\nif __name__ == \\'__main__\\':\\n  import argparse\\n  import sys\\n  from pyspark import SparkConf, SparkContext\\n  from tensorflowonspark import TFCluster\\n\\n  sc = SparkContext(conf=SparkConf().setAppName(\\'wide_deep\\'))\\n  executors = int(sc._conf.get(\"spark.executor.instances\", \"1\"))\\n\\n  # arguments for Spark and TFoS\\n  parser = argparse.ArgumentParser()\\n  parser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=executors)\\n  parser.add_argument(\"--num_ps\", help=\"number of ps nodes\", type=int, default=1)\\n  (args, remainder) = parser.parse_known_args()\\n\\n  # construct an ARGV (with script name as first element) from remaining args and pass it to the TF processes on executors\\n  remainder.insert(0, __file__)\\n  print(\"spark args:\", args)\\n  print(\"tf args:\", remainder)\\n\\n  num_workers = args.cluster_size - args.num_ps\\n  print(\"===== num_executors={}, num_workers={}, num_ps={}\".format(args.cluster_size, num_workers, args.num_ps))\\n\\n  cluster = TFCluster.run(sc, main_fun, remainder, args.cluster_size, args.num_ps, False, TFCluster.InputMode.TENSORFLOW, master_node=\\'master\\')\\n  cluster.shutdown()\\n',\n",
              " 'wide_deep_run_loop.py': b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n# ==============================================================================\\n\"\"\"Core run logic for TensorFlow Wide & Deep Tutorial using tf.estimator API.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport os\\nimport shutil\\n\\nfrom absl import app as absl_app\\nfrom absl import flags\\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\\n\\nfrom official.utils.flags import core as flags_core\\nfrom official.utils.logs import hooks_helper\\nfrom official.utils.logs import logger\\nfrom official.utils.misc import model_helpers\\n\\n\\nLOSS_PREFIX = {\\'wide\\': \\'linear/\\', \\'deep\\': \\'dnn/\\'}\\n\\n\\ndef define_wide_deep_flags():\\n  \"\"\"Add supervised learning flags, as well as wide-deep model type.\"\"\"\\n  flags_core.define_base()\\n  flags_core.define_benchmark()\\n  flags_core.define_performance(\\n      num_parallel_calls=False, inter_op=True, intra_op=True,\\n      synthetic_data=False, max_train_steps=False, dtype=False,\\n      all_reduce_alg=False)\\n\\n  flags.adopt_module_key_flags(flags_core)\\n  flags.DEFINE_integer(name=\\'train_epochs\\',default=40,help=\\'to define epochs\\')\\n  flags.DEFINE_integer(name=\\'epochs_between_evals\\',default=2,help=\\'to define epochs between evals\\')\\n  flags.DEFINE_string(name=\\'clean\\',default=\\'model_dir\\',help=\\'to define clean model_dir\\')\\n  flags.DEFINE_list(name=\\'hooks\\',default=[\\'LoggingTensorHook\\',\\'stepcounterhook\\'],help=\\'to define hooks\\')\\n\\n  flags.DEFINE_enum(\\n      name=\"model_type\", short_name=\"mt\", default=\"wide_deep\",\\n      enum_values=[\\'wide\\', \\'deep\\', \\'wide_deep\\'],\\n      help=\"Select model topology.\")\\n  flags.DEFINE_boolean(\\n      name=\"download_if_missing\", default=True, help=flags_core.help_wrap(\\n          \"Download data to data_dir if it is not already present.\"))\\n\\n\\ndef export_model(model, model_type, export_dir, model_column_fn):\\n  \"\"\"Export to SavedModel format.\\n\\n  Args:\\n    model: Estimator object\\n    model_type: string indicating model type. \"wide\", \"deep\" or \"wide_deep\"\\n    export_dir: directory to export the model.\\n    model_column_fn: Function to generate model feature columns.\\n  \"\"\"\\n  wide_columns, deep_columns = model_column_fn()\\n  if model_type == \\'wide\\':\\n    columns = wide_columns\\n  elif model_type == \\'deep\\':\\n    columns = deep_columns\\n  else:\\n    columns = wide_columns + deep_columns\\n  feature_spec = tf.feature_column.make_parse_example_spec(columns)\\n  example_input_fn = (\\n      tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec))\\n  model.export_savedmodel(export_dir, example_input_fn,\\n                          strip_default_attrs=True)\\n\\n\\ndef run_loop(name, train_input_fn, eval_input_fn, model_column_fn,\\n             build_estimator_fn, flags_obj, tensors_to_log, early_stop=False):\\n  \"\"\"Define training loop.\"\"\"\\n  model_helpers.apply_clean(flags.FLAGS)\\n  model = build_estimator_fn(\\n      model_dir=flags_obj.model_dir, model_type=flags_obj.model_type,\\n      model_column_fn=model_column_fn,\\n      inter_op=flags_obj.inter_op_parallelism_threads,\\n      intra_op=flags_obj.intra_op_parallelism_threads)\\n\\n  run_params = {\\n      \\'batch_size\\': flags_obj.batch_size,\\n      \\'train_epochs\\': flags_obj.train_epochs,\\n      \\'model_type\\': flags_obj.model_type,\\n  }\\n\\n  benchmark_logger = logger.get_benchmark_logger()\\n  benchmark_logger.log_run_info(\\'wide_deep\\', name, run_params,\\n                                test_id=flags_obj.benchmark_test_id)\\n\\n  loss_prefix = LOSS_PREFIX.get(flags_obj.model_type, \\'\\')\\n  tensors_to_log = {k: v.format(loss_prefix=loss_prefix)\\n                    for k, v in tensors_to_log.items()}\\n  logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=10)\\n\\n\\n  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\\n  for n in range(flags_obj.train_epochs // flags_obj.epochs_between_evals):\\n    model.train(input_fn=train_input_fn, hooks=[logging_hook])\\n\\n    results = model.evaluate(input_fn=eval_input_fn)\\n\\n    # Display evaluation metrics\\n    tf.logging.info(\\'Results at epoch %d / %d\\',\\n                    (n + 1) * flags_obj.epochs_between_evals,\\n                    flags_obj.train_epochs)\\n    tf.logging.info(\\'-\\' * 60)\\n    print(\\'Results at epoch %d / %d\\',\\n          (n + 1) * flags_obj.epochs_between_evals,\\n          flags_obj.train_epochs)\\n    for key in sorted(results):\\n      tf.logging.info(\\'%s: %s\\' % (key, results[key]))\\n\\n    benchmark_logger.log_evaluation_result(results)\\n\\t\\n    print(results)\\n    if early_stop and model_helpers.past_stop_threshold(\\n        flags_obj.stop_threshold, results[\\'accuracy\\']):\\n      break\\n\\n  # Export the model\\n  if flags_obj.export_dir is not None:\\n    export_model(model, flags_obj.model_type, flags_obj.export_dir,\\n                 model_column_fn)\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up842sOP2auO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train_dataset.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMicUz2v2d9u",
        "colab_type": "code",
        "outputId": "46335637-750c-4b94-ee61-2999d8001ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!${SPARK_HOME}/bin/spark-submit \\\n",
        "--master spark://423fe09a2473:7077 \\\n",
        "--py-files train_dataset.py,wide_deep_run_loop.py \\\n",
        "--conf spark.cores.max=3 \\\n",
        "--conf spark.task.cpus=1 \\\n",
        "--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\n",
        "--conf spark.task.maxFailures=1 \\\n",
        "--conf spark.stage.maxConsecutiveAttempts=1 \\\n",
        "train_main.py \\\n",
        "--cluster_size 3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/10/15 06:40:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "19/10/15 06:40:54 INFO SparkContext: Running Spark version 2.4.4\n",
            "19/10/15 06:40:55 INFO SparkContext: Submitted application: wide_deep\n",
            "19/10/15 06:40:55 INFO SecurityManager: Changing view acls to: root\n",
            "19/10/15 06:40:55 INFO SecurityManager: Changing modify acls to: root\n",
            "19/10/15 06:40:55 INFO SecurityManager: Changing view acls groups to: \n",
            "19/10/15 06:40:55 INFO SecurityManager: Changing modify acls groups to: \n",
            "19/10/15 06:40:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "19/10/15 06:40:55 INFO Utils: Successfully started service 'sparkDriver' on port 41063.\n",
            "19/10/15 06:40:55 INFO SparkEnv: Registering MapOutputTracker\n",
            "19/10/15 06:40:55 INFO SparkEnv: Registering BlockManagerMaster\n",
            "19/10/15 06:40:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "19/10/15 06:40:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "19/10/15 06:40:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b897403c-30f7-42d7-92e3-4b1dde7d1625\n",
            "19/10/15 06:40:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
            "19/10/15 06:40:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "19/10/15 06:40:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "19/10/15 06:40:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://423fe09a2473:4040\n",
            "19/10/15 06:40:55 INFO SparkContext: Added file file:///root/recommender/TensorFlowOnSpark/examples/train_dataset.py at spark://423fe09a2473:41063/files/train_dataset.py with timestamp 1571121655795\n",
            "19/10/15 06:40:55 INFO Utils: Copying /root/recommender/TensorFlowOnSpark/examples/train_dataset.py to /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/userFiles-a0679be5-5e29-4d4c-8641-c3ff3dd3a741/train_dataset.py\n",
            "19/10/15 06:40:55 INFO SparkContext: Added file file:///root/recommender/TensorFlowOnSpark/examples/wide_deep_run_loop.py at spark://423fe09a2473:41063/files/wide_deep_run_loop.py with timestamp 1571121655811\n",
            "19/10/15 06:40:55 INFO Utils: Copying /root/recommender/TensorFlowOnSpark/examples/wide_deep_run_loop.py to /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/userFiles-a0679be5-5e29-4d4c-8641-c3ff3dd3a741/wide_deep_run_loop.py\n",
            "19/10/15 06:40:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://423fe09a2473:7077...\n",
            "19/10/15 06:40:56 INFO TransportClientFactory: Successfully created connection to 423fe09a2473/172.28.0.2:7077 after 43 ms (0 ms spent in bootstraps)\n",
            "19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191015064056-0003\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/0 on worker-20191015052724-172.28.0.2-43191 (172.28.0.2:43191) with 1 core(s)\n",
            "19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/0 on hostPort 172.28.0.2:43191 with 1 core(s), 1024.0 MB RAM\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/1 on worker-20191015052719-172.28.0.2-43969 (172.28.0.2:43969) with 1 core(s)\n",
            "19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/1 on hostPort 172.28.0.2:43969 with 1 core(s), 1024.0 MB RAM\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/2 on worker-20191015052722-172.28.0.2-36691 (172.28.0.2:36691) with 1 core(s)\n",
            "19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/2 on hostPort 172.28.0.2:36691 with 1 core(s), 1024.0 MB RAM\n",
            "19/10/15 06:40:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35399.\n",
            "19/10/15 06:40:56 INFO NettyBlockTransferService: Server created on 423fe09a2473:35399\n",
            "19/10/15 06:40:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/0 is now RUNNING\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/1 is now RUNNING\n",
            "19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/2 is now RUNNING\n",
            "19/10/15 06:40:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 423fe09a2473, 35399, None)\n",
            "19/10/15 06:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 423fe09a2473:35399 with 366.3 MB RAM, BlockManagerId(driver, 423fe09a2473, 35399, None)\n",
            "19/10/15 06:40:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 423fe09a2473, 35399, None)\n",
            "19/10/15 06:40:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 423fe09a2473, 35399, None)\n",
            "19/10/15 06:40:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
            "spark args: Namespace(cluster_size=3, num_ps=1)\n",
            "tf args: ['/root/recommender/TensorFlowOnSpark/examples/train_main.py']\n",
            "===== num_executors=3, num_workers=2, num_ps=1\n",
            "2019-10-15 06:40:56,773 INFO (MainThread-3730) Reserving TFSparkNodes \n",
            "2019-10-15 06:40:56,773 INFO (MainThread-3730) cluster_template: {'ps': [0], 'master': [1], 'worker': [2]}\n",
            "2019-10-15 06:40:56,775 INFO (MainThread-3730) listening for reservations at ('172.28.0.2', 45731)\n",
            "2019-10-15 06:40:56,784 INFO (MainThread-3730) Starting TensorFlow on executors\n",
            "2019-10-15 06:40:57,634 INFO (MainThread-3730) Waiting for TFSparkNodes to start\n",
            "2019-10-15 06:40:57,634 INFO (MainThread-3730) waiting for 3 reservations\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 590, in dumps\n",
            "    return cloudpickle.dumps(obj, 2)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 863, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 260, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 409, in dump\n",
            "    self.save(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 751, in save_tuple\n",
            "    save(element)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\n",
            "    save(x)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\n",
            "    save(x)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\n",
            "    save(x)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\n",
            "    save(x)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 808, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 808, in _batch_appends\n",
            "    save(tmp[0])\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\n",
            "    self._batch_appends(obj)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\n",
            "    save(x)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 400, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
            "    save(state)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
            "    self._batch_setitems(obj.items())\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
            "    save(v)\n",
            "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
            "    f(self, obj) # Call unbound method with explicit self\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 400, in save_function\n",
            "    self.save_function_tuple(obj)\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 526, in save_function_tuple\n",
            "    itertools.chain(f_globals.values(), closure_values or ()),\n",
            "  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 431, in _save_subimports\n",
            "    for name, module in sys.modules.items():\n",
            "RuntimeError: dictionary changed size during iteration\n",
            "2019-10-15 06:40:57,683 ERROR (Thread-3-3730) Exception in TF background thread\n",
            "2019-10-15 06:40:58,635 INFO (MainThread-3730) waiting for 3 reservations\n",
            "19/10/15 06:40:58 INFO SparkUI: Stopped Spark web UI at http://423fe09a2473:4040\n",
            "19/10/15 06:40:58 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
            "19/10/15 06:40:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
            "19/10/15 06:40:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "19/10/15 06:40:58 INFO MemoryStore: MemoryStore cleared\n",
            "19/10/15 06:40:58 INFO BlockManager: BlockManager stopped\n",
            "19/10/15 06:40:58 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "19/10/15 06:40:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "19/10/15 06:40:58 INFO SparkContext: Successfully stopped SparkContext\n",
            "19/10/15 06:40:59 INFO ShutdownHookManager: Shutdown hook called\n",
            "19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3b8f5ed-ff45-4a36-9764-8cc3128b023a\n",
            "19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/pyspark-3fb32a38-e47e-4068-92c9-81643819b25c\n",
            "19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc\n",
            "19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/localPyFiles-804b7a62-6d29-4206-9e6b-1025255d55f9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C15JL51HE3Qi",
        "colab_type": "code",
        "outputId": "3525d976-0832-415e-c9c8-c3c18607e12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow' from '/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Teua8SYo0Pbe",
        "colab_type": "code",
        "outputId": "a260d30e-df72-446d-c9f8-35e2f7dd476b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# pylint: disable=wrong-import-order\n",
        "from absl import app as absl_app\n",
        "from absl import flags\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "# pylint: enable=wrong-import-order\n",
        "\n",
        "from official.utils.flags import core as flags_core\n",
        "\n",
        "\n",
        "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult'\n",
        "TRAINING_FILE = 'adult.data'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_FILE = 'adult.test'\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
        "\n",
        "\n",
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
        "                        [0], [0], [0], [''], ['']]\n",
        "\n",
        "_HASH_BUCKET_SIZE = 1000\n",
        "\n",
        "_NUM_EXAMPLES = {\n",
        "    'train': 32561,\n",
        "    'validation': 16281,\n",
        "}\n",
        "\n",
        "\n",
        "def _download_and_clean_file(filename, url):\n",
        "  \"\"\"Downloads data from url, and makes changes to match the CSV format.\"\"\"\n",
        "  temp_file, _ = urllib.request.urlretrieve(url)\n",
        "  with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n",
        "    with tf.gfile.Open(filename, 'w') as eval_file:\n",
        "      for line in temp_eval_file:\n",
        "        line = line.strip()\n",
        "        line = line.replace(', ', ',')\n",
        "        if not line or ',' not in line:\n",
        "          continue\n",
        "        if line[-1] == '.':\n",
        "          line = line[:-1]\n",
        "        line += '\\n'\n",
        "        eval_file.write(line)\n",
        "  tf.gfile.Remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "  \"\"\"Download census data if it is not already present.\"\"\"\n",
        "  tf.gfile.MakeDirs(data_dir)\n",
        "\n",
        "  training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "  if not tf.gfile.Exists(training_file_path):\n",
        "    _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "  eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "  if not tf.gfile.Exists(eval_file_path):\n",
        "    _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "\n",
        "def build_model_columns():\n",
        "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
        "  # Continuous variable columns\n",
        "  age = tf.feature_column.numeric_column('age')\n",
        "  education_num = tf.feature_column.numeric_column('education_num')\n",
        "  capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
        "  capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
        "  hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
        "\n",
        "  education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'education', [\n",
        "          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
        "          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
        "          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
        "\n",
        "  marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'marital_status', [\n",
        "          'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
        "          'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
        "\n",
        "  relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'relationship', [\n",
        "          'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
        "          'Other-relative'])\n",
        "\n",
        "  workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'workclass', [\n",
        "          'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
        "          'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
        "\n",
        "  # To show an example of hashing:\n",
        "  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "      'occupation', hash_bucket_size=_HASH_BUCKET_SIZE)\n",
        "\n",
        "  # Transformations.\n",
        "  age_buckets = tf.feature_column.bucketized_column(\n",
        "      age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "\n",
        "  # Wide columns and deep columns.\n",
        "  base_columns = [\n",
        "      education, marital_status, relationship, workclass, occupation,\n",
        "      age_buckets,\n",
        "  ]\n",
        "\n",
        "  crossed_columns = [\n",
        "      tf.feature_column.crossed_column(\n",
        "          ['education', 'occupation'], hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "      tf.feature_column.crossed_column(\n",
        "          [age_buckets, 'education', 'occupation'],\n",
        "          hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "  ]\n",
        "\n",
        "  wide_columns = base_columns + crossed_columns\n",
        "\n",
        "  deep_columns = [\n",
        "      age,\n",
        "      education_num,\n",
        "      capital_gain,\n",
        "      capital_loss,\n",
        "      hours_per_week,\n",
        "      tf.feature_column.indicator_column(workclass),\n",
        "      tf.feature_column.indicator_column(education),\n",
        "      tf.feature_column.indicator_column(marital_status),\n",
        "      tf.feature_column.indicator_column(relationship),\n",
        "      # To show an example of embedding\n",
        "      tf.feature_column.embedding_column(occupation, dimension=8),\n",
        "  ]\n",
        "\n",
        "  return wide_columns, deep_columns\n",
        "\n",
        "\n",
        "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
        "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
        "  assert tf.gfile.Exists(data_file), (\n",
        "      '%s not found. Please make sure you have run census_dataset.py and '\n",
        "      'set the --data_dir argument to the correct path.' % data_file)\n",
        "\n",
        "  def parse_csv(value):\n",
        "    tf.logging.info('Parsing {}'.format(data_file))\n",
        "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
        "    features = dict(zip(_CSV_COLUMNS, columns))\n",
        "    labels = features.pop('income_bracket')\n",
        "    classes = tf.equal(labels, '>50K')  # binary classification\n",
        "    return features, classes\n",
        "\n",
        "  # Extract lines from input files using the Dataset API.\n",
        "  dataset = tf.data.TextLineDataset(data_file)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
        "\n",
        "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
        "\n",
        "  # We call repeat after shuffling, rather than before, to prevent separate\n",
        "  # epochs from blending together.\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def define_data_download_flags():\n",
        "  \"\"\"Add flags specifying data download arguments.\"\"\"\n",
        "  flags.DEFINE_string(\n",
        "      name=\"data_dir\", default=\"/tmp/census_data/\",\n",
        "      help=flags_core.help_wrap(\n",
        "          \"Directory to download and extract data.\"))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  download(flags.FLAGS.data_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "  flags.FLAGS(sys.argv)\n",
        "  define_data_download_flags()\n",
        "  absl_app.run(main)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYncRj1dYu_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11wkFfDoen_m",
        "colab_type": "code",
        "outputId": "8c1d0140-0452-47de-b03b-3d5071ae87de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "%tb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ba17ef6399f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0mdefine_data_download_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m   \u001b[0mabsl_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8oXHsXX9pBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# pylint: disable=wrong-import-order\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "# pylint: enable=wrong-import-order\n",
        "\n",
        "\n",
        "\n",
        "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult'\n",
        "TRAINING_FILE = 'adult.data'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_FILE = 'adult.test'\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
        "\n",
        "\n",
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
        "                        [0], [0], [0], [''], ['']]\n",
        "\n",
        "_HASH_BUCKET_SIZE = 1000\n",
        "\n",
        "_NUM_EXAMPLES = {\n",
        "    'train': 32561,\n",
        "    'validation': 16281,\n",
        "}\n",
        "\n",
        "\n",
        "def _download_and_clean_file(filename, url):\n",
        "  \"\"\"Downloads data from url, and makes changes to match the CSV format.\"\"\"\n",
        "  temp_file, _ = urllib.request.urlretrieve(url)\n",
        "  with tf.io.gfile.GFile(temp_file, 'r') as temp_eval_file:\n",
        "    with tf.io.gfile.GFile(filename, 'w') as eval_file:\n",
        "      for line in temp_eval_file:\n",
        "        line = line.strip()\n",
        "        line = line.replace(', ', ',')\n",
        "        if not line or ',' not in line:\n",
        "          continue\n",
        "        if line[-1] == '.':\n",
        "          line = line[:-1]\n",
        "        line += '\\n'\n",
        "        eval_file.write(line)\n",
        "  tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "  \"\"\"Download census data if it is not already present.\"\"\"\n",
        "  tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "  training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "  if not tf.io.gfile.exists(training_file_path):\n",
        "    _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "  eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "  if not tf.io.gfile.exists(eval_file_path):\n",
        "    _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "\n",
        "def build_model_columns():\n",
        "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
        "  # Continuous variable columns\n",
        "  age = tf.feature_column.numeric_column('age')\n",
        "  education_num = tf.feature_column.numeric_column('education_num')\n",
        "  capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
        "  capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
        "  hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
        "\n",
        "  education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'education', [\n",
        "          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
        "          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
        "          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
        "\n",
        "  marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'marital_status', [\n",
        "          'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
        "          'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
        "\n",
        "  relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'relationship', [\n",
        "          'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
        "          'Other-relative'])\n",
        "\n",
        "  workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'workclass', [\n",
        "          'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
        "          'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
        "\n",
        "  # To show an example of hashing:\n",
        "  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "      'occupation', hash_bucket_size=_HASH_BUCKET_SIZE)\n",
        "\n",
        "  # Transformations.\n",
        "  age_buckets = tf.feature_column.bucketized_column(\n",
        "      age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "\n",
        "  # Wide columns and deep columns.\n",
        "  base_columns = [\n",
        "      education, marital_status, relationship, workclass, occupation,\n",
        "      age_buckets,\n",
        "  ]\n",
        "\n",
        "  crossed_columns = [\n",
        "      tf.feature_column.crossed_column(\n",
        "          ['education', 'occupation'], hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "      tf.feature_column.crossed_column(\n",
        "          [age_buckets, 'education', 'occupation'],\n",
        "          hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "  ]\n",
        "\n",
        "  wide_columns = base_columns + crossed_columns\n",
        "\n",
        "  deep_columns = [\n",
        "      age,\n",
        "      education_num,\n",
        "      capital_gain,\n",
        "      capital_loss,\n",
        "      hours_per_week,\n",
        "      tf.feature_column.indicator_column(workclass),\n",
        "      tf.feature_column.indicator_column(education),\n",
        "      tf.feature_column.indicator_column(marital_status),\n",
        "      tf.feature_column.indicator_column(relationship),\n",
        "      # To show an example of embedding\n",
        "      tf.feature_column.embedding_column(occupation, dimension=8),\n",
        "  ]\n",
        "\n",
        "  return wide_columns, deep_columns\n",
        "\n",
        "\n",
        "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
        "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
        "  assert tf.io.gfile.exists(data_file), (\n",
        "      '%s not found. Please make sure you have run census_dataset.py and '\n",
        "      'set the --data_dir argument to the correct path.' % data_file)\n",
        "\n",
        "  def parse_csv(value):\n",
        "    tf.logging.info('Parsing {}'.format(data_file))\n",
        "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
        "    features = dict(zip(_CSV_COLUMNS, columns))\n",
        "    labels = features.pop('Saving_Account')\n",
        "    classes = tf.equal(labels, 0)  # binary classification\n",
        "    return features, classes\n",
        "\n",
        "  # Extract lines from input files using the Dataset API.\n",
        "  dataset = tf.data.TextLineDataset(data_file)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
        "\n",
        "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
        "\n",
        "  # We call repeat after shuffling, rather than before, to prevent separate\n",
        "  # epochs from blending together.\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  download('/root/recommender/TensorFlowOnSpark/examples/wide_deep')\n",
        "\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  main(_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzxW8E5raJgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download('/root/recommender/TensorFlowOnSpark/examples/wide_deep')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkwNQ7t59p6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_input_fn():\n",
        "  return input_fn('/root/recommender/TensorFlowOnSpark/examples/wide_deep/adult.data',10,False,100)\n",
        "def eval_input_fn():\n",
        "  return input_fn('/root/recommender/TensorFlowOnSpark/examples/wide_deep/adult.test',10,False,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAYt6Ew_ISW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\n",
        "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_CZ3_isSGXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_estimator(model_dir, model_type, model_column_fn):\n",
        "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
        "  wide_columns, deep_columns = model_column_fn()\n",
        "  hidden_units = [100, 75, 50, 25]\n",
        "\n",
        "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
        "  # trains faster than GPU for this model.\n",
        "  if model_type == 'wide':\n",
        "    return tf.estimator.LinearClassifier(\n",
        "        model_dir=model_dir,\n",
        "        feature_columns=wide_columns)\n",
        "  elif model_type == 'deep':\n",
        "    return tf.estimator.DNNClassifier(\n",
        "        model_dir=model_dir,\n",
        "        feature_columns=deep_columns,\n",
        "        hidden_units=hidden_units,\n",
        "        config=run_config)\n",
        "  else:\n",
        "    return tf.estimator.DNNLinearCombinedClassifier(\n",
        "        model_dir=model_dir,\n",
        "        linear_feature_columns=wide_columns,\n",
        "        dnn_feature_columns=deep_columns,\n",
        "        dnn_hidden_units=hidden_units,\n",
        "        config=run_config)\n",
        "run_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={'GPU': 0},inter_op_parallelism_threads=0,intra_op_parallelism_threads=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCAH-39saTvw",
        "colab_type": "code",
        "outputId": "3e8108b4-6395-446d-d377-295e4d062331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_estimator(model_dir='/root/recommender/TensorFlowOnSpark/examples/wide_deep/', model_type='deep',model_column_fn=build_model_columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/root/recommender/TensorFlowOnSpark/examples/wide_deep/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
            "  key: \"GPU\"\n",
            "  value: 0\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9546b18b38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-174f80265150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/root/recommender/TensorFlowOnSpark/examples/wide_deep/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deep'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_column_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-180-1f4b2330e2a8>\u001b[0m in \u001b[0;36mbuild_estimator\u001b[0;34m(model_dir, model_type, model_column_fn)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         config=run_config)\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     return tf.estimator.DNNLinearCombinedClassifier(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_units, feature_columns, model_dir, n_classes, weight_column, label_vocabulary, optimizer, activation_fn, dropout, input_layer_partitioner, config, warm_start_from, loss_reduction, batch_norm)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         warm_start_from=warm_start_from)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_fn, model_dir, config, params, warm_start_from)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using config: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     self._device_fn = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/tf_logging.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logging.info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \"\"\"\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36m_log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         record = self.makeRecord(self.name, level, fn, lno, msg, args,\n\u001b[1;32m   1443\u001b[0m                                  exc_info, func, extra, sinfo)\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \"\"\"\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallHandlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mcallHandlers\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevelno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                     \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m    \u001b[0;31m#break out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0m_warn_preinit_stderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_to_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logtostderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_to_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_hide_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'logtostderr'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYgUxqRJZnVJ",
        "colab_type": "code",
        "outputId": "cc6f76e5-3d79-4ea9-9cfa-2dd03cfef345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-d630bd28d0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;31m# If `distribute_coordinator_mode` is set and running in distributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;31m# environment, we run `train_and_evaluate` via distribute coordinator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mdistribute_coordinator_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run_distribute_coordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running `train_and_evaluate` with Distribute Coordinator.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     distribute_coordinator_training.train_and_evaluate(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/estimator_training.py\u001b[0m in \u001b[0;36mshould_run_distribute_coordinator\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    184\u001b[0m   if (not hasattr(config, '_distribute_coordinator_mode') or\n\u001b[1;32m    185\u001b[0m       config._distribute_coordinator_mode is None):\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not using Distribute Coordinator.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   if (not isinstance(config._distribute_coordinator_mode, six.string_types) or\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/tf_logging.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logging.info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \"\"\"\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36m_log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         record = self.makeRecord(self.name, level, fn, lno, msg, args,\n\u001b[1;32m   1443\u001b[0m                                  exc_info, func, extra, sinfo)\n\u001b[0;32m-> 1444\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \"\"\"\n\u001b[1;32m   1453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallHandlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mcallHandlers\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevelno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m                     \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m    \u001b[0;31m#break out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36memit\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0m_warn_preinit_stderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_to_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logtostderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_to_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_hide_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'logtostderr'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSfHSe56d3qO",
        "colab_type": "code",
        "outputId": "e24babfc-22bd-452a-d7f1-c1e5a87f57ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int)\n",
        "parser.add_argument(\"--num_ps\", help=\"number of ps nodes\", type=int, default=1)\n",
        "(args, remainder) = parser.parse_known_args()\n",
        "cluster = TFCluster.run(sc, main,remainder,2, 1, False, TFCluster.InputMode.TENSORFLOW, master_node='spark://6a2ec83c491b:7077')\n",
        "cluster.shutdown()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-17 05:02:44,845 INFO (MainThread-4739) Reserving TFSparkNodes \n",
            "2019-09-17 05:02:44,846 INFO (MainThread-4739) cluster_template: {'ps': [0], 'spark://6a2ec83c491b:7077': [1]}\n",
            "2019-09-17 05:02:44,886 INFO (MainThread-4739) listening for reservations at ('172.28.0.2', 35751)\n",
            "2019-09-17 05:02:44,892 INFO (MainThread-4739) Starting TensorFlow on executors\n",
            "2019-09-17 05:02:45,142 INFO (MainThread-4739) Waiting for TFSparkNodes to start\n",
            "2019-09-17 05:02:45,144 INFO (MainThread-4739) waiting for 2 reservations\n",
            "2019-09-17 05:02:45,228 ERROR (Thread-6-4739) Exception in TF background thread\n",
            "2019-09-17 05:02:46,147 INFO (MainThread-4739) waiting for 2 reservations\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP7sYmsWnGo1",
        "colab_type": "code",
        "outputId": "56e0c5aa-8dce-4171-d829-52f5f7f5d84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /root/recommender"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/recommender\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJiw8HuoIQyv",
        "colab_type": "code",
        "outputId": "ef53d49f-15bb-45c5-b33a-ad36dbb2f91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "data=pd.read_csv('train_ver2.csv','r',encoding='utf=8',delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etxV70OTcTN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headers=['Partitioned Date','Customer code','Employee index','Country residence','sex','Age','Date of First holder','New customer Index','Customer seniority','Primary Customer','Last date as primary customer','type at beginning','relation at beginning','Residence index','Foreigner index','Spouse index','channel to join','Deceased index','Addres type','Province code','Province name','Activity index','Gross income','segmentation','Saving Account','Guarantees','Current Accounts','Derivada Account','Payroll Account','Junior Account','Más particular Account','particular Account','particular Plus Account','Short-term deposits','Medium-term deposits','Long-term deposits','e-account','Funds','Mortgage','Pensions','Loans','Taxes','Credit Card','Securities','Home Account','Payroll','Pensions2','Direct Debit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRd4KFGaHSvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns=headers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9iDtye6Pn-_",
        "colab_type": "code",
        "outputId": "22a51627-54d7-4348-90ac-60939394de56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "del(data['Employee index'])\n",
        "del(data['Country residence'])\n",
        "del(data['Date of First holder'])\n",
        "del(data['Primary Customer'])\n",
        "del(data['Last date as primary customer'])\n",
        "del(data['type at beginning'])\n",
        "del(data['Residence index'])\n",
        "del(data['Spouse index'])\n",
        "del(data['Deceased index'])\n",
        "del(data['Addres type'])\n",
        "del(data['Province name'])\n",
        "data['Total Purchases']=data.iloc[:,-24:].sum(axis=1)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Partitioned Date</th>\n",
              "      <th>Customer code</th>\n",
              "      <th>sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>New customer Index</th>\n",
              "      <th>Customer seniority</th>\n",
              "      <th>relation at beginning</th>\n",
              "      <th>Foreigner index</th>\n",
              "      <th>channel to join</th>\n",
              "      <th>Province code</th>\n",
              "      <th>Activity index</th>\n",
              "      <th>Gross income</th>\n",
              "      <th>segmentation</th>\n",
              "      <th>Saving Account</th>\n",
              "      <th>Guarantees</th>\n",
              "      <th>Current Accounts</th>\n",
              "      <th>Derivada Account</th>\n",
              "      <th>Payroll Account</th>\n",
              "      <th>Junior Account</th>\n",
              "      <th>Más particular Account</th>\n",
              "      <th>particular Account</th>\n",
              "      <th>particular Plus Account</th>\n",
              "      <th>Short-term deposits</th>\n",
              "      <th>Medium-term deposits</th>\n",
              "      <th>Long-term deposits</th>\n",
              "      <th>e-account</th>\n",
              "      <th>Funds</th>\n",
              "      <th>Mortgage</th>\n",
              "      <th>Pensions</th>\n",
              "      <th>Loans</th>\n",
              "      <th>Taxes</th>\n",
              "      <th>Credit Card</th>\n",
              "      <th>Securities</th>\n",
              "      <th>Home Account</th>\n",
              "      <th>Payroll</th>\n",
              "      <th>Pensions2</th>\n",
              "      <th>Direct Debit</th>\n",
              "      <th>Total Purchases</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1375586</td>\n",
              "      <td>H</td>\n",
              "      <td>35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHL</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>87218.10</td>\n",
              "      <td>02 - PARTICULARES</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050611</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35548.74</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050612</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>122179.11</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050613</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHD</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>119775.54</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050614</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050615</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22220.04</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050616</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>295590.36</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050617</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113316.66</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050619</td>\n",
              "      <td>H</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050620</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113194.98</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050621</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72575.88</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050622</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050623</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>49.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>113538.81</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050624</td>\n",
              "      <td>H</td>\n",
              "      <td>65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>61605.09</td>\n",
              "      <td>02 - PARTICULARES</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050625</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>49.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050626</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050610</td>\n",
              "      <td>V</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>37.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>68318.46</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050627</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65608.35</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050609</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KFA</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>73432.47</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050605</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050582</td>\n",
              "      <td>V</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>64620.57</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050586</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>64194.99</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050588</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050589</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>119173.89</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050591</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>58728.39</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050592</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHD</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050595</td>\n",
              "      <td>V</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>86863.38</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050596</td>\n",
              "      <td>H</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>S</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>68421.36</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050597</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2015-01-28</td>\n",
              "      <td>1050598</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>90408.75</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647279</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166759</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161098.71</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647280</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166761</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647281</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166762</td>\n",
              "      <td>V</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99712.89</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647282</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166787</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647283</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166786</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647284</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166785</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>89725.05</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647285</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166784</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126939.30</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647286</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166783</td>\n",
              "      <td>H</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101259.96</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647287</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166782</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647288</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166781</td>\n",
              "      <td>H</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66748.50</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647289</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166780</td>\n",
              "      <td>H</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58274.22</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647290</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166779</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>57302.85</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647291</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166778</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647292</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166777</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>157609.83</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647293</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166776</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647294</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166775</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>77784.78</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647295</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166774</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160865.58</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647296</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166773</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>83822.43</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647297</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166772</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100154.25</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647298</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166771</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647299</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166770</td>\n",
              "      <td>H</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85873.14</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647300</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166769</td>\n",
              "      <td>H</td>\n",
              "      <td>26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47093.04</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647301</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166768</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65849.55</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647302</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166767</td>\n",
              "      <td>V</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73134.81</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647303</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166766</td>\n",
              "      <td>V</td>\n",
              "      <td>25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50945.25</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647304</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166765</td>\n",
              "      <td>V</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43912.17</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647305</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166764</td>\n",
              "      <td>V</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23334.99</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647306</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166763</td>\n",
              "      <td>H</td>\n",
              "      <td>47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>A</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02 - PARTICULARES</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647307</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1166789</td>\n",
              "      <td>H</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>I</td>\n",
              "      <td>N</td>\n",
              "      <td>KHE</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>199592.82</td>\n",
              "      <td>03 - UNIVERSITARIO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13647308</th>\n",
              "      <td>2016-05-28</td>\n",
              "      <td>1550586</td>\n",
              "      <td>H</td>\n",
              "      <td>37</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13647309 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Partitioned Date  Customer code  ... Direct Debit Total Purchases\n",
              "0              2015-01-28        1375586  ...            0             1.0\n",
              "1              2015-01-28        1050611  ...            0             1.0\n",
              "2              2015-01-28        1050612  ...            0             1.0\n",
              "3              2015-01-28        1050613  ...            0             1.0\n",
              "4              2015-01-28        1050614  ...            0             1.0\n",
              "5              2015-01-28        1050615  ...            0             1.0\n",
              "6              2015-01-28        1050616  ...            0             1.0\n",
              "7              2015-01-28        1050617  ...            0             1.0\n",
              "8              2015-01-28        1050619  ...            0             1.0\n",
              "9              2015-01-28        1050620  ...            0             1.0\n",
              "10             2015-01-28        1050621  ...            0             1.0\n",
              "11             2015-01-28        1050622  ...            0             1.0\n",
              "12             2015-01-28        1050623  ...            0             1.0\n",
              "13             2015-01-28        1050624  ...            0             1.0\n",
              "14             2015-01-28        1050625  ...            0             1.0\n",
              "15             2015-01-28        1050626  ...            0             1.0\n",
              "16             2015-01-28        1050610  ...            0             1.0\n",
              "17             2015-01-28        1050627  ...            0             1.0\n",
              "18             2015-01-28        1050609  ...            0             1.0\n",
              "19             2015-01-28        1050605  ...            0             1.0\n",
              "20             2015-01-28        1050582  ...            0             1.0\n",
              "21             2015-01-28        1050586  ...            1             2.0\n",
              "22             2015-01-28        1050588  ...            0             1.0\n",
              "23             2015-01-28        1050589  ...            0             1.0\n",
              "24             2015-01-28        1050591  ...            0             1.0\n",
              "25             2015-01-28        1050592  ...            0             1.0\n",
              "26             2015-01-28        1050595  ...            0             1.0\n",
              "27             2015-01-28        1050596  ...            0             1.0\n",
              "28             2015-01-28        1050597  ...            0             1.0\n",
              "29             2015-01-28        1050598  ...            0             1.0\n",
              "...                   ...            ...  ...          ...             ...\n",
              "13647279       2016-05-28        1166759  ...            0             1.0\n",
              "13647280       2016-05-28        1166761  ...            0             1.0\n",
              "13647281       2016-05-28        1166762  ...            0             0.0\n",
              "13647282       2016-05-28        1166787  ...            0             1.0\n",
              "13647283       2016-05-28        1166786  ...            0             1.0\n",
              "13647284       2016-05-28        1166785  ...            0             1.0\n",
              "13647285       2016-05-28        1166784  ...            0             1.0\n",
              "13647286       2016-05-28        1166783  ...            0             1.0\n",
              "13647287       2016-05-28        1166782  ...            0             1.0\n",
              "13647288       2016-05-28        1166781  ...            0             1.0\n",
              "13647289       2016-05-28        1166780  ...            0             1.0\n",
              "13647290       2016-05-28        1166779  ...            0             1.0\n",
              "13647291       2016-05-28        1166778  ...            0             1.0\n",
              "13647292       2016-05-28        1166777  ...            0             1.0\n",
              "13647293       2016-05-28        1166776  ...            0             1.0\n",
              "13647294       2016-05-28        1166775  ...            0             1.0\n",
              "13647295       2016-05-28        1166774  ...            0             1.0\n",
              "13647296       2016-05-28        1166773  ...            0             0.0\n",
              "13647297       2016-05-28        1166772  ...            0             1.0\n",
              "13647298       2016-05-28        1166771  ...            0             1.0\n",
              "13647299       2016-05-28        1166770  ...            0             1.0\n",
              "13647300       2016-05-28        1166769  ...            0             1.0\n",
              "13647301       2016-05-28        1166768  ...            0             1.0\n",
              "13647302       2016-05-28        1166767  ...            0             1.0\n",
              "13647303       2016-05-28        1166766  ...            0             1.0\n",
              "13647304       2016-05-28        1166765  ...            0             1.0\n",
              "13647305       2016-05-28        1166764  ...            0             1.0\n",
              "13647306       2016-05-28        1166763  ...            0             1.0\n",
              "13647307       2016-05-28        1166789  ...            0             1.0\n",
              "13647308       2016-05-28        1550586  ...            0             2.0\n",
              "\n",
              "[13647309 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgKl4v2VgALR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_encode(df, features, name):\n",
        "    df[name] = df[name].astype('str')\n",
        "    if name in transformers: # test\n",
        "        df[name] = transformers[name].transform(df[name])\n",
        "    else: # train\n",
        "        transformers[name] = sklearn.preprocessing.LabelEncoder()\n",
        "        df[name] = transformers[name].fit_transform(df[name])\n",
        "    features.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwOpo5ckRkHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features=[]\n",
        "transformers={}\n",
        "data[\"code\"]=data[\"Customer code\"]\n",
        "data[\"Gross income\"].fillna(1.0, inplace=True)\n",
        "data[\"Gross income\"]=data[\"Gross income\"].replace('         NA',data[\"Gross income\"].value_counts().index[0])\n",
        "data['Gross income']=data['Gross income'].map(lambda x :float(x))\n",
        "data[\"Customer seniority\"]=data[\"Customer seniority\"].replace('     NA',0)\n",
        "data['Customer seniority']=data['Customer seniority'].map(lambda x :int(x))\n",
        "data[\"Customer seniority\"]=data[\"Customer seniority\"].replace(-999999,0)\n",
        "data['Customer seniority']=data['Customer seniority'].map(lambda x :int(x))\n",
        "data['sex'].fillna(data['sex'].value_counts().index[0],inplace=True)\n",
        "data['Age']=data['Age'].replace(' NA',data['Age'].value_counts().index[0])\n",
        "data['Age']=data['Age'].map(lambda x :int(x))\n",
        "data['relation at beginning'].fillna(data['relation at beginning'].value_counts().index[0],inplace=True)\n",
        "data['channel to join'].fillna(data['channel to join'].value_counts().index[0],inplace=True)\n",
        "data['Foreigner index'].fillna(data['Foreigner index'].value_counts().index[0],inplace=True)\n",
        "data['Province code'].fillna(data['Province code'].value_counts().index[0],inplace=True)\n",
        "data['Activity index'].fillna(data['Province code'].value_counts().index[0],inplace=True)\n",
        "data['segmentation'].fillna(data['segmentation'].value_counts().index[0],inplace=True)\n",
        "label_encode(data, features, \"channel to join\") # simple label encode\n",
        "# label_encode(data, features, \"nomprov\") # use cod_prov only\n",
        "label_encode(data, features, 'Activity index')\n",
        "label_encode(data, features, \"Foreigner index\")\n",
        "label_encode(data, features, \"sex\",)\n",
        "label_encode(data, features, \"segmentation\")\n",
        "label_encode(data, features, \"relation at beginning\")\n",
        "label_encode(data, features, \"Customer code\")\n",
        "data['New customer Index'].fillna(0,inplace=True)\n",
        "data[\"Partitioned Month\"] = data[\"Partitioned Date\"].map(lambda x: int(x.split(\"-\")[1])).astype(np.int8)\n",
        "data[\"Partitioned Month\"]=data[\"Partitioned Month\"].map(lambda x:x-1)\n",
        "data[\"Partitioned Year\"] = data[\"Partitioned Date\"].map(lambda x: int(x.split(\"-\")[0])).astype(np.int64)\n",
        "label_encode(data, features, \"Partitioned Year\")\n",
        "del(data[\"Partitioned Date\"])\n",
        "data['New customer Index']=data['New customer Index'].map(lambda x :int(x))\n",
        "data['Province code']=data['Province code'].map(lambda x :int(x))\n",
        "data['Gross income']=data['Gross income'].map(lambda x :int(x))\n",
        "data['Payroll'].fillna(0,inplace=True)\n",
        "data['Pensions2'].fillna(0,inplace=True)\n",
        "data['Payroll']=data['Payroll'].map(lambda x :int(x))\n",
        "data['Pensions2']=data['Pensions2'].map(lambda x :int(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpjCd5RKONVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data[['Partitioned Month','Partitioned Year','sex','Age','New customer Index','Customer seniority','relation at beginning','Foreigner index','Province code','Activity index','Gross income','segmentation','Saving Account','Guarantees','Current Accounts','Derivada Account','Payroll Account','Junior Account','Más particular Account','particular Account','particular Plus Account','Short-term deposits','Medium-term deposits','Long-term deposits','e-account','Funds','Mortgage','Pensions','Loans','Taxes','Credit Card','Securities','Home Account','Payroll','Pensions2','Direct Debit']]\n",
        "header=['Partitioned_Month','Partitioned_Year','sex','Age','New_customer_Index','Customer_seniority','relation_at_beginning','Foreigner_index','Province_code','Activity_index','Gross_income','segmentation','Saving_Account','Guarantees','Current Accounts','Derivada Account','Payroll Account','Junior Account','Más particular Account','particular Account','particular Plus Account','Short-term deposits','Medium-term deposits','Long-term deposits','e-account','Funds','Mortgage','Pensions','Loans','Taxes','Credit Card','Securities','Home Account','Payroll','Pensions2','Direct Debit']\n",
        "data.columns=header\n",
        "train=data[:13647309-900000]\n",
        "test=data[13647309-900000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAXlyoaQ5BBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('train.csv',index=False,header=False)\n",
        "test.to_csv('test.csv',index=False,header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zfrsPH84pCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('train.csv','r',encoding='utf=8',delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Aws0q9pYToR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=pd.read_csv('test.csv','r',encoding='utf=8',delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZvjK3M8YPwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata=train.iloc[:,0:12]\n",
        "train_labels=train.iloc[:,12:]\n",
        "evaldata=test.iloc[:,0:12]\n",
        "eval_labels=test.iloc[:,12:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYk9JZQhjY3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# pylint: disable=wrong-import-order\n",
        "from absl import app as absl_app\n",
        "from absl import flags\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "# pylint: enable=wrong-import-order\n",
        "\n",
        "from official.utils.flags import core as flags_core\n",
        "\n",
        "\n",
        "\n",
        "_CSV_COLUMNS = ['Partitioned_Month','Partitioned_Year','sex','Age','New_customer_Index','Customer_seniority','relation_at_beginning','Foreigner_index','Province_code','Activity_index','Gross_income','segmentation','Saving_Account','Guarantees','Current Accounts','Derivada Account','Payroll Account','Junior Account','Más particular Account','particular Account','particular Plus Account','Short-term deposits','Medium-term deposits','Long-term deposits','e-account','Funds','Mortgage','Pensions','Loans','Taxes','Credit Card','Securities','Home Account','Payroll','Pensions2','Direct Debit']\n",
        "\n",
        "_CSV_COLUMN_DEFAULTS = [[0], [0], [0], [0], [0], [0],[0] ,[0], [0], [0], [0], [0], [0],[0] [0], [0], [0], [0], [0],[0], [0], [0], [0], [0], [0], [0], [0], [0],[0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
        "\n",
        "_HASH_BUCKET_SIZE = 1000\n",
        "\n",
        "_NUM_EXAMPLES = {\n",
        "    'train': 12747308,\n",
        "    'validation': 900000,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_model_columns():\n",
        "  \"\"\"Builds a set of wide and deep feature columns.\"\"\"\n",
        "  # Continuous variable columns\n",
        "  Age = tf.feature_column.numeric_column('Age')\n",
        "  Customer_seniority = tf.feature_column.numeric_column('Customer_seniority')\n",
        "  Gross_income = tf.feature_column.numeric_column('Gross_income')\n",
        "\n",
        "  Partitioned_Month = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Partitioned_Month', [0,1,2,3,4,5,6,7,8,9,10,11])\n",
        "\n",
        "  Partitioned_Year = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Partitioned_Year', [0,1])\n",
        "\n",
        "  sex = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'sex', [0,1])\n",
        "\n",
        "  New_customer_Index = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'New_customer_Index', [0,1])\n",
        "\n",
        "  relation_at_beginning = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'relation_at_beginning', [0,1])\n",
        "  \n",
        "  Foreigner_index = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Foreigner_index', [0,1])\t  \n",
        "\t  \n",
        "  Activity_index = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Activity_index', [0,1,2])\n",
        "\t  \n",
        "  Province_code = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Province_code', [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53])\n",
        "\n",
        "  segmentation = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'segmentation', [0,1,2])\n",
        "\n",
        "  # To show an example of hashing:\n",
        "  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "      'occupation', hash_bucket_size=_HASH_BUCKET_SIZE)\n",
        "\n",
        "  \n",
        "  # Transformations.\n",
        "  age_buckets = tf.feature_column.bucketized_column(\n",
        "      Age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "\n",
        "  # Wide columns and deep columns.\n",
        "  base_columns = [\n",
        "      Partitioned_Month, Partitioned_Year, sex, New_customer_Index, relation_at_beginning,Foreigner_index,Province_code,segmentation,\n",
        "      age_buckets,\n",
        "  ]\n",
        "  crossed_columns = [\n",
        "      tf.feature_column.crossed_column(\n",
        "          ['education', 'occupation'], hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "      tf.feature_column.crossed_column(\n",
        "          [age_buckets, 'education', 'occupation'],\n",
        "          hash_bucket_size=_HASH_BUCKET_SIZE),\n",
        "  ]\n",
        "\n",
        "\n",
        "  wide_columns = base_columns\n",
        "\n",
        "  deep_columns = [\n",
        "      Age,\n",
        "      Customer_seniority,\n",
        "      Gross_income,\n",
        "      tf.feature_column.indicator_column(Partitioned_Month),\n",
        "      tf.feature_column.indicator_column(Partitioned_Year),\n",
        "      tf.feature_column.indicator_column(sex),\n",
        "      tf.feature_column.indicator_column(New_customer_Index),\n",
        "\t    tf.feature_column.indicator_column(relation_at_beginning),\n",
        "\t    tf.feature_column.indicator_column(Foreigner_index),\n",
        "\t    tf.feature_column.indicator_column(Province_code),\n",
        "\t    tf.feature_column.indicator_column(segmentation)]\n",
        "\n",
        "  return wide_columns, deep_columns\n",
        "\n",
        "\n",
        "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
        "  \n",
        "  assert tf.gfile.Exists(data_file), (\n",
        "      '%s not found. Please make sure you have run train_dataset.py and '\n",
        "      'set the --data_dir argument to the correct path.' % data_file)\n",
        "\n",
        "  def parse_csv(value):\n",
        "    tf.logging.info('Parsing {}'.format(data_file))\n",
        "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
        "    features = dict(zip(_CSV_COLUMNS, columns))\n",
        "    labels = features.pop('Saving_Account')\n",
        "    classes = tf.equal(labels,0)  # binary classification\n",
        "    return features, classes\n",
        "\n",
        "  # Extract lines from input files using the Dataset API.\n",
        "  dataset = tf.data.TextLineDataset(data_file)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
        "\n",
        "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
        "\n",
        "  # We call repeat after shuffling, rather than before, to prevent separate\n",
        "  # epochs from blending together.\n",
        "  dataset = dataset.repeat(num_epochs)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def define_data_download_flags():\n",
        "  \"\"\"Add flags specifying data download arguments.\"\"\"\n",
        "  flags.DEFINE_string(\n",
        "      name=\"data_dir\", default=\"/root/recommender/train_data.csv\",\n",
        "      help=flags_core.help_wrap(\n",
        "          \"Directory to download and extract data.\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LosAz5TUpYpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_input_fn():\n",
        "  return input_fn('/root/recommender/train.csv',5,False,10000)\n",
        "def eval_input_fn():\n",
        "  return input_fn('/root/recommender/test.csv',5,False,10000)\n",
        "train_spec1 = tf.estimator.TrainSpec(input_fn=train_input_fn)\n",
        "eval_spec1 = tf.estimator.EvalSpec(input_fn=eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sECY8QardNDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_estimator(model_dir, model_type, model_column_fn):\n",
        "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
        "  wide_columns, deep_columns = model_column_fn()\n",
        "  hidden_units = [100, 75, 50, 25]\n",
        "\n",
        "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
        "  # trains faster than GPU for this model.\n",
        "  if model_type == 'wide':\n",
        "    return tf.estimator.LinearClassifier(\n",
        "        model_dir=model_dir,\n",
        "        feature_columns=wide_columns)\n",
        "  elif model_type == 'deep':\n",
        "    return tf.estimator.DNNClassifier(\n",
        "        model_dir=model_dir,\n",
        "        feature_columns=deep_columns,\n",
        "        hidden_units=hidden_units,\n",
        "        config=run_config)\n",
        "  else:\n",
        "    return tf.estimator.DNNLinearCombinedClassifier(\n",
        "        model_dir=model_dir,\n",
        "        linear_feature_columns=wide_columns,\n",
        "        dnn_feature_columns=deep_columns,\n",
        "        dnn_hidden_units=hidden_units,\n",
        "        config=run_config)\n",
        "run_config = tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={'GPU': 0},inter_op_parallelism_threads=0,intra_op_parallelism_threads=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Dq2xSaal4W",
        "colab_type": "code",
        "outputId": "3526e3ed-2ed6-4db1-9f21-8e4748083fd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "model = build_estimator(model_dir='/root/recommender/', model_type='deep',model_column_fn=build_model_columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/root/recommender/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
            "  key: \"GPU\"\n",
            "  value: 0\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbdcbedf7f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:11,492 INFO (MainThread-144) Using config: {'_model_dir': '/root/recommender/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
            "  key: \"GPU\"\n",
            "  value: 0\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbdcbedf7f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSO7guaoau7X",
        "colab_type": "code",
        "outputId": "1bfddbdc-30e6-42da-a353-798203d63ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.estimator.train_and_evaluate(model, train_spec1, eval_spec1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:13,914 INFO (MainThread-144) Not using Distribute Coordinator.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:13,917 INFO (MainThread-144) Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:13,921 INFO (MainThread-144) Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Parsing /root/recommender/train.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:13,953 INFO (MainThread-144) Parsing /root/recommender/train.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,011 INFO (MainThread-144) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,660 INFO (MainThread-144) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,663 INFO (MainThread-144) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,866 INFO (MainThread-144) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/recommender/model.ckpt-561117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,872 INFO (MainThread-144) Restoring parameters from /root/recommender/model.ckpt-561117\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,941 WARNING (MainThread-144) From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:14,983 INFO (MainThread-144) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:15,011 INFO (MainThread-144) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 561117 into /root/recommender/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:15,494 INFO (MainThread-144) Saving checkpoints for 561117 into /root/recommender/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.5456548, step = 561118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:10:16,643 INFO (MainThread-144) loss = 1.5456548, step = 561118\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.54407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:11:21,406 INFO (MainThread-144) global_step/sec: 1.54407\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.527249, step = 561218 (64.766 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:11:21,409 INFO (MainThread-144) loss = 4.527249, step = 561218 (64.766 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.55225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:12:25,829 INFO (MainThread-144) global_step/sec: 1.55225\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 2.6426833, step = 561318 (64.424 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:12:25,833 INFO (MainThread-144) loss = 2.6426833, step = 561318 (64.424 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.53844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:13:30,830 INFO (MainThread-144) global_step/sec: 1.53844\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.675765, step = 561418 (65.002 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:13:30,834 INFO (MainThread-144) loss = 5.675765, step = 561418 (65.002 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.5537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:14:35,193 INFO (MainThread-144) global_step/sec: 1.5537\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 21.53196, step = 561518 (64.362 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:14:35,196 INFO (MainThread-144) loss = 21.53196, step = 561518 (64.362 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.54247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:15:40,024 INFO (MainThread-144) global_step/sec: 1.54247\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.9696608, step = 561618 (64.831 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:15:40,027 INFO (MainThread-144) loss = 1.9696608, step = 561618 (64.831 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.54682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:16:44,672 INFO (MainThread-144) global_step/sec: 1.54682\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.1038678, step = 561718 (64.649 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:16:44,676 INFO (MainThread-144) loss = 1.1038678, step = 561718 (64.649 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.53641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:17:49,759 INFO (MainThread-144) global_step/sec: 1.53641\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.714167, step = 561818 (65.086 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:17:49,762 INFO (MainThread-144) loss = 4.714167, step = 561818 (65.086 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.5335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:18:54,969 INFO (MainThread-144) global_step/sec: 1.5335\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.1350561, step = 561918 (65.211 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:18:54,973 INFO (MainThread-144) loss = 1.1350561, step = 561918 (65.211 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.54494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:19:59,697 INFO (MainThread-144) global_step/sec: 1.54494\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 3.564447, step = 562018 (64.728 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:19:59,701 INFO (MainThread-144) loss = 3.564447, step = 562018 (64.728 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 562043 into /root/recommender/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:16,043 INFO (MainThread-144) Saving checkpoints for 562043 into /root/recommender/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Parsing /root/recommender/test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:16,160 INFO (MainThread-144) Parsing /root/recommender/test.csv\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:16,217 INFO (MainThread-144) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,434 WARNING (MainThread-144) Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,461 WARNING (MainThread-144) Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,484 INFO (MainThread-144) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-10-14T09:20:17Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,506 INFO (MainThread-144) Starting evaluation at 2019-10-14T09:20:17Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,650 INFO (MainThread-144) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/recommender/model.ckpt-562043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,657 INFO (MainThread-144) Restoring parameters from /root/recommender/model.ckpt-562043\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,775 INFO (MainThread-144) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:17,836 INFO (MainThread-144) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [10/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:24,675 INFO (MainThread-144) Evaluation [10/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [20/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:31,158 INFO (MainThread-144) Evaluation [20/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:37,789 INFO (MainThread-144) Evaluation [30/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:44,308 INFO (MainThread-144) Evaluation [40/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [50/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:50,927 INFO (MainThread-144) Evaluation [50/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [60/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:20:57,378 INFO (MainThread-144) Evaluation [60/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [70/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:03,906 INFO (MainThread-144) Evaluation [70/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [80/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:10,531 INFO (MainThread-144) Evaluation [80/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [90/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:17,072 INFO (MainThread-144) Evaluation [90/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [100/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:23,560 INFO (MainThread-144) Evaluation [100/100]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-10-14-09:21:23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:23,652 INFO (MainThread-144) Finished evaluation at 2019-10-14-09:21:23\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 562043: accuracy = 0.999909, accuracy_baseline = 0.999909, auc = 0.495084, auc_precision_recall = 0.9999531, average_loss = 0.0017494889, global_step = 562043, label/mean = 0.999909, loss = 17.494888, precision = 0.999909, prediction/mean = 0.9992147, recall = 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:23,654 INFO (MainThread-144) Saving dict for global step 562043: accuracy = 0.999909, accuracy_baseline = 0.999909, auc = 0.495084, auc_precision_recall = 0.9999531, average_loss = 0.0017494889, global_step = 562043, label/mean = 0.999909, loss = 17.494888, precision = 0.999909, prediction/mean = 0.9992147, recall = 1.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 562043: /root/recommender/model.ckpt-562043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:21:23,657 INFO (MainThread-144) Saving 'checkpoint_path' summary for global step 562043: /root/recommender/model.ckpt-562043\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.75389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:22:12,342 INFO (MainThread-144) global_step/sec: 0.75389\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.6496248, step = 562118 (132.647 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:22:12,347 INFO (MainThread-144) loss = 1.6496248, step = 562118 (132.647 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.53941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:23:17,302 INFO (MainThread-144) global_step/sec: 1.53941\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.1683497, step = 562218 (64.959 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:23:17,306 INFO (MainThread-144) loss = 1.1683497, step = 562218 (64.959 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.53155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:24:22,596 INFO (MainThread-144) global_step/sec: 1.53155\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.4249592, step = 562318 (65.293 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-14 09:24:22,599 INFO (MainThread-144) loss = 1.4249592, step = 562318 (65.293 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-c47171408f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    471\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    612\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1260\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9aKmSwmhlUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn_model_fn(features, labels, mode, num_classes=24):\n",
        "\n",
        "    # Input Layer\n",
        "    input_layer = tf.reshape(features['x'], [-1,1000,12])\n",
        "\n",
        "\n",
        "    # Convolutional Layer #1\n",
        "    conv1 = tf.layers.conv1d(\n",
        "        inputs=input_layer,\n",
        "        filters=32,\n",
        "        kernel_size=5,\n",
        "        padding=\"same\",\n",
        "        activation=tf.nn.relu)\n",
        "\n",
        "    # Pooling Layer #1\n",
        "    pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2)\n",
        "\n",
        "    # Convolutional Layer #2 and Pooling Layer #2\n",
        "    conv2 = tf.layers.conv1d(\n",
        "        inputs=pool1,\n",
        "        filters=64,\n",
        "        kernel_size=5,\n",
        "        padding=\"same\",\n",
        "        activation=tf.nn.relu)\n",
        "\n",
        "    # Pooling Layer #2\n",
        "    pool2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2)\n",
        "\n",
        "    # Dense Layer\n",
        "    pool2_flat = tf.reshape(pool2, [-1,16])\n",
        "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    # Logits Layer\n",
        "    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n",
        "\n",
        "    predictions = {\"probabilities\": tf.sigmoid(logits, name=\"sigmoid_tensor\")}\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "    loss = tf.identity(tf.losses.sigmoid_cross_entropy(labels, logits=logits), name='loss')\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        tf.summary.scalar('train loss', loss)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "    #return tf.estimator.EstimatorSpec(mode=mode, loss=tf.identity(tf.losses.sigmoid_cross_entropy(labels, logits=logits)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Cisf4D9P99",
        "colab_type": "code",
        "outputId": "92efbfe0-870a-49bb-c848-0b29a4580123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": traindata.values.astype(float)},y=train_labels.values,batch_size=1000,num_epochs=10,shuffle=True)\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": evaldata.values.astype(float)},y=eval_labels.values,batch_size=1000,num_epochs=1,shuffle=False)\n",
        "classifier=tf.estimator.Estimator(model_fn=partial(cnn_model_fn,num_classes=train_labels.shape[1]),model_dir=\"./pascal2_models/\")\n",
        "classifier.train(input_fn=train_input_fn, steps=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,588 INFO (MainThread-139) Using default config.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': './pascal2_models/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1297e87588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,590 INFO (MainThread-139) Using config: {'_model_dir': './pascal2_models/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1297e87588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,623 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,768 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,943 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:30,948 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,044 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,055 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-1900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,110 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,117 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,328 INFO (MainThread-139) Saving checkpoints for 1900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.5112838745117188, step = 1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:31,492 INFO (MainThread-139) loss = 0.5112838745117188, step = 1900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 103.309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:32,459 INFO (MainThread-139) global_step/sec: 103.309\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4988531172275543, step = 2000 (0.971 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:32,463 INFO (MainThread-139) loss = 0.4988531172275543, step = 2000 (0.971 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:33,356 INFO (MainThread-139) global_step/sec: 111.506\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.5022936463356018, step = 2100 (0.898 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:33,360 INFO (MainThread-139) loss = 0.5022936463356018, step = 2100 (0.898 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:34,254 INFO (MainThread-139) global_step/sec: 111.301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3587583005428314, step = 2200 (0.897 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:34,257 INFO (MainThread-139) loss = 0.3587583005428314, step = 2200 (0.897 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:35,142 INFO (MainThread-139) global_step/sec: 112.679\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4985564649105072, step = 2300 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:35,146 INFO (MainThread-139) loss = 0.4985564649105072, step = 2300 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:36,034 INFO (MainThread-139) global_step/sec: 112.092\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.48231324553489685, step = 2400 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:36,037 INFO (MainThread-139) loss = 0.48231324553489685, step = 2400 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:36,938 INFO (MainThread-139) global_step/sec: 110.635\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4671218991279602, step = 2500 (0.904 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:36,941 INFO (MainThread-139) loss = 0.4671218991279602, step = 2500 (0.904 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:37,835 INFO (MainThread-139) global_step/sec: 111.414\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4571770429611206, step = 2600 (0.897 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:37,838 INFO (MainThread-139) loss = 0.4571770429611206, step = 2600 (0.897 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:38,738 INFO (MainThread-139) global_step/sec: 110.816\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.42098164558410645, step = 2700 (0.903 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:38,742 INFO (MainThread-139) loss = 0.42098164558410645, step = 2700 (0.903 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:39,632 INFO (MainThread-139) global_step/sec: 111.752\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.29012295603752136, step = 2800 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:39,636 INFO (MainThread-139) loss = 0.29012295603752136, step = 2800 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:40,536 INFO (MainThread-139) global_step/sec: 110.716\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4115934371948242, step = 2900 (0.906 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:40,542 INFO (MainThread-139) loss = 0.4115934371948242, step = 2900 (0.906 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:41,433 INFO (MainThread-139) global_step/sec: 111.478\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3953981101512909, step = 3000 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:41,437 INFO (MainThread-139) loss = 0.3953981101512909, step = 3000 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:42,335 INFO (MainThread-139) global_step/sec: 110.859\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4489493668079376, step = 3100 (0.900 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:42,337 INFO (MainThread-139) loss = 0.4489493668079376, step = 3100 (0.900 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:43,230 INFO (MainThread-139) global_step/sec: 111.695\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3776439130306244, step = 3200 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:43,233 INFO (MainThread-139) loss = 0.3776439130306244, step = 3200 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:44,130 INFO (MainThread-139) global_step/sec: 111.066\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.37036678194999695, step = 3300 (0.900 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:44,133 INFO (MainThread-139) loss = 0.37036678194999695, step = 3300 (0.900 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:45,036 INFO (MainThread-139) global_step/sec: 110.368\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4399973452091217, step = 3400 (0.906 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:45,039 INFO (MainThread-139) loss = 0.4399973452091217, step = 3400 (0.906 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:45,932 INFO (MainThread-139) global_step/sec: 111.621\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3837810456752777, step = 3500 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:45,936 INFO (MainThread-139) loss = 0.3837810456752777, step = 3500 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:46,826 INFO (MainThread-139) global_step/sec: 111.845\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3950372636318207, step = 3600 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:46,831 INFO (MainThread-139) loss = 0.3950372636318207, step = 3600 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:47,723 INFO (MainThread-139) global_step/sec: 111.499\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.39492684602737427, step = 3700 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:47,727 INFO (MainThread-139) loss = 0.39492684602737427, step = 3700 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:48,616 INFO (MainThread-139) global_step/sec: 112.072\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.4321266710758209, step = 3800 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:48,618 INFO (MainThread-139) loss = 0.4321266710758209, step = 3800 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:49,512 INFO (MainThread-139) global_step/sec: 111.496\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.39817920327186584, step = 3900 (0.897 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:49,515 INFO (MainThread-139) loss = 0.39817920327186584, step = 3900 (0.897 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:50,408 INFO (MainThread-139) global_step/sec: 111.722\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.32999417185783386, step = 4000 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:50,411 INFO (MainThread-139) loss = 0.32999417185783386, step = 4000 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:51,299 INFO (MainThread-139) global_step/sec: 112.116\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3247585892677307, step = 4100 (0.893 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:51,304 INFO (MainThread-139) loss = 0.3247585892677307, step = 4100 (0.893 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:52,189 INFO (MainThread-139) global_step/sec: 112.363\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.36918964982032776, step = 4200 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:52,193 INFO (MainThread-139) loss = 0.36918964982032776, step = 4200 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:53,080 INFO (MainThread-139) global_step/sec: 112.316\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.37025272846221924, step = 4300 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:53,083 INFO (MainThread-139) loss = 0.37025272846221924, step = 4300 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:53,968 INFO (MainThread-139) global_step/sec: 112.536\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.34823232889175415, step = 4400 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:53,973 INFO (MainThread-139) loss = 0.34823232889175415, step = 4400 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:54,864 INFO (MainThread-139) global_step/sec: 111.593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3636927902698517, step = 4500 (0.898 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:54,870 INFO (MainThread-139) loss = 0.3636927902698517, step = 4500 (0.898 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:55,759 INFO (MainThread-139) global_step/sec: 111.805\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.345913290977478, step = 4600 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:55,762 INFO (MainThread-139) loss = 0.345913290977478, step = 4600 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:56,651 INFO (MainThread-139) global_step/sec: 112.15\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2935783565044403, step = 4700 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:56,653 INFO (MainThread-139) loss = 0.2935783565044403, step = 4700 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:57,537 INFO (MainThread-139) global_step/sec: 112.808\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.29488638043403625, step = 4800 (0.887 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:57,540 INFO (MainThread-139) loss = 0.29488638043403625, step = 4800 (0.887 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:58,429 INFO (MainThread-139) global_step/sec: 112.095\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.34717807173728943, step = 4900 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:58,432 INFO (MainThread-139) loss = 0.34717807173728943, step = 4900 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:59,321 INFO (MainThread-139) global_step/sec: 112.091\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3344367444515228, step = 5000 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:38:59,324 INFO (MainThread-139) loss = 0.3344367444515228, step = 5000 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:00,209 INFO (MainThread-139) global_step/sec: 112.605\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3958728313446045, step = 5100 (0.888 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:00,212 INFO (MainThread-139) loss = 0.3958728313446045, step = 5100 (0.888 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:01,103 INFO (MainThread-139) global_step/sec: 111.872\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28629741072654724, step = 5200 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:01,106 INFO (MainThread-139) loss = 0.28629741072654724, step = 5200 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:01,996 INFO (MainThread-139) global_step/sec: 111.991\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28541386127471924, step = 5300 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:02,000 INFO (MainThread-139) loss = 0.28541386127471924, step = 5300 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:02,888 INFO (MainThread-139) global_step/sec: 112.067\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2718135416507721, step = 5400 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:02,891 INFO (MainThread-139) loss = 0.2718135416507721, step = 5400 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:03,782 INFO (MainThread-139) global_step/sec: 111.946\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.27080217003822327, step = 5500 (0.893 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:03,784 INFO (MainThread-139) loss = 0.27080217003822327, step = 5500 (0.893 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:04,663 INFO (MainThread-139) global_step/sec: 113.507\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2657136917114258, step = 5600 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:04,669 INFO (MainThread-139) loss = 0.2657136917114258, step = 5600 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:05,559 INFO (MainThread-139) global_step/sec: 111.531\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26433348655700684, step = 5700 (0.893 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:05,562 INFO (MainThread-139) loss = 0.26433348655700684, step = 5700 (0.893 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.568\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:06,448 INFO (MainThread-139) global_step/sec: 112.568\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.273534893989563, step = 5800 (0.890 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:06,453 INFO (MainThread-139) loss = 0.273534893989563, step = 5800 (0.890 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:07,344 INFO (MainThread-139) global_step/sec: 111.582\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3316360414028168, step = 5900 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:07,349 INFO (MainThread-139) loss = 0.3316360414028168, step = 5900 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:08,231 INFO (MainThread-139) global_step/sec: 112.711\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2923468053340912, step = 6000 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:08,234 INFO (MainThread-139) loss = 0.2923468053340912, step = 6000 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:09,124 INFO (MainThread-139) global_step/sec: 112.048\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3301447033882141, step = 6100 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:09,127 INFO (MainThread-139) loss = 0.3301447033882141, step = 6100 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:10,024 INFO (MainThread-139) global_step/sec: 111.031\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.29575857520103455, step = 6200 (0.901 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:10,027 INFO (MainThread-139) loss = 0.29575857520103455, step = 6200 (0.901 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:10,918 INFO (MainThread-139) global_step/sec: 111.904\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.24458736181259155, step = 6300 (0.893 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:10,921 INFO (MainThread-139) loss = 0.24458736181259155, step = 6300 (0.893 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:11,811 INFO (MainThread-139) global_step/sec: 111.923\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.23578564822673798, step = 6400 (0.893 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:11,814 INFO (MainThread-139) loss = 0.23578564822673798, step = 6400 (0.893 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:12,705 INFO (MainThread-139) global_step/sec: 111.894\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.33821046352386475, step = 6500 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:12,708 INFO (MainThread-139) loss = 0.33821046352386475, step = 6500 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:13,589 INFO (MainThread-139) global_step/sec: 113.125\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.37069734930992126, step = 6600 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:13,593 INFO (MainThread-139) loss = 0.37069734930992126, step = 6600 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:14,490 INFO (MainThread-139) global_step/sec: 111.039\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3174724578857422, step = 6700 (0.901 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:14,495 INFO (MainThread-139) loss = 0.3174724578857422, step = 6700 (0.901 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:15,382 INFO (MainThread-139) global_step/sec: 112.011\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28397077322006226, step = 6800 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:15,386 INFO (MainThread-139) loss = 0.28397077322006226, step = 6800 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:16,270 INFO (MainThread-139) global_step/sec: 112.727\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3071083724498749, step = 6900 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:16,275 INFO (MainThread-139) loss = 0.3071083724498749, step = 6900 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:17,162 INFO (MainThread-139) global_step/sec: 111.988\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.287236750125885, step = 7000 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:17,166 INFO (MainThread-139) loss = 0.287236750125885, step = 7000 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.779\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:18,065 INFO (MainThread-139) global_step/sec: 110.779\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3487459421157837, step = 7100 (0.902 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:18,068 INFO (MainThread-139) loss = 0.3487459421157837, step = 7100 (0.902 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:18,959 INFO (MainThread-139) global_step/sec: 111.907\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.30174705386161804, step = 7200 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:18,962 INFO (MainThread-139) loss = 0.30174705386161804, step = 7200 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:19,851 INFO (MainThread-139) global_step/sec: 112.035\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.225254625082016, step = 7300 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:19,854 INFO (MainThread-139) loss = 0.225254625082016, step = 7300 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:20,739 INFO (MainThread-139) global_step/sec: 112.68\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2850353717803955, step = 7400 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:20,743 INFO (MainThread-139) loss = 0.2850353717803955, step = 7400 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:21,636 INFO (MainThread-139) global_step/sec: 111.519\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21863116323947906, step = 7500 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:21,638 INFO (MainThread-139) loss = 0.21863116323947906, step = 7500 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:22,535 INFO (MainThread-139) global_step/sec: 111.126\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2124272882938385, step = 7600 (0.902 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:22,540 INFO (MainThread-139) loss = 0.2124272882938385, step = 7600 (0.902 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:23,443 INFO (MainThread-139) global_step/sec: 110.139\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21952831745147705, step = 7700 (0.906 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:23,446 INFO (MainThread-139) loss = 0.21952831745147705, step = 7700 (0.906 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:24,345 INFO (MainThread-139) global_step/sec: 110.858\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.29134103655815125, step = 7800 (0.903 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:24,349 INFO (MainThread-139) loss = 0.29134103655815125, step = 7800 (0.903 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:25,238 INFO (MainThread-139) global_step/sec: 111.991\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28500643372535706, step = 7900 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:25,241 INFO (MainThread-139) loss = 0.28500643372535706, step = 7900 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:26,135 INFO (MainThread-139) global_step/sec: 111.585\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2157241553068161, step = 8000 (0.897 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:26,138 INFO (MainThread-139) loss = 0.2157241553068161, step = 8000 (0.897 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:27,025 INFO (MainThread-139) global_step/sec: 112.327\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21503068506717682, step = 8100 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:27,029 INFO (MainThread-139) loss = 0.21503068506717682, step = 8100 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:27,918 INFO (MainThread-139) global_step/sec: 111.97\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.23787076771259308, step = 8200 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:27,921 INFO (MainThread-139) loss = 0.23787076771259308, step = 8200 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:28,813 INFO (MainThread-139) global_step/sec: 111.732\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.32423135638237, step = 8300 (0.896 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:28,817 INFO (MainThread-139) loss = 0.32423135638237, step = 8300 (0.896 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:29,699 INFO (MainThread-139) global_step/sec: 112.838\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2922554612159729, step = 8400 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:29,702 INFO (MainThread-139) loss = 0.2922554612159729, step = 8400 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:30,588 INFO (MainThread-139) global_step/sec: 112.512\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2878836393356323, step = 8500 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:30,592 INFO (MainThread-139) loss = 0.2878836393356323, step = 8500 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:31,471 INFO (MainThread-139) global_step/sec: 113.191\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2561093270778656, step = 8600 (0.884 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:31,476 INFO (MainThread-139) loss = 0.2561093270778656, step = 8600 (0.884 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:32,355 INFO (MainThread-139) global_step/sec: 113.201\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.30934444069862366, step = 8700 (0.881 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:32,357 INFO (MainThread-139) loss = 0.30934444069862366, step = 8700 (0.881 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:33,252 INFO (MainThread-139) global_step/sec: 111.402\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.31270256638526917, step = 8800 (0.904 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:33,261 INFO (MainThread-139) loss = 0.31270256638526917, step = 8800 (0.904 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 110.288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:34,159 INFO (MainThread-139) global_step/sec: 110.288\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.20109745860099792, step = 8900 (0.901 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:34,161 INFO (MainThread-139) loss = 0.20109745860099792, step = 8900 (0.901 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:35,037 INFO (MainThread-139) global_step/sec: 113.876\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2380814254283905, step = 9000 (0.880 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:35,041 INFO (MainThread-139) loss = 0.2380814254283905, step = 9000 (0.880 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:35,925 INFO (MainThread-139) global_step/sec: 112.667\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.19284042716026306, step = 9100 (0.888 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:35,930 INFO (MainThread-139) loss = 0.19284042716026306, step = 9100 (0.888 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:36,814 INFO (MainThread-139) global_step/sec: 112.429\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2210935652256012, step = 9200 (0.890 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:36,820 INFO (MainThread-139) loss = 0.2210935652256012, step = 9200 (0.890 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:37,708 INFO (MainThread-139) global_step/sec: 111.924\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1853470504283905, step = 9300 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:37,712 INFO (MainThread-139) loss = 0.1853470504283905, step = 9300 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:38,594 INFO (MainThread-139) global_step/sec: 112.805\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1933651864528656, step = 9400 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:38,597 INFO (MainThread-139) loss = 0.1933651864528656, step = 9400 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:39,481 INFO (MainThread-139) global_step/sec: 112.723\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.19654834270477295, step = 9500 (0.887 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:39,484 INFO (MainThread-139) loss = 0.19654834270477295, step = 9500 (0.887 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:40,377 INFO (MainThread-139) global_step/sec: 111.672\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.15531262755393982, step = 9600 (0.898 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:40,382 INFO (MainThread-139) loss = 0.15531262755393982, step = 9600 (0.898 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:41,269 INFO (MainThread-139) global_step/sec: 112.044\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2537594735622406, step = 9700 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:41,274 INFO (MainThread-139) loss = 0.2537594735622406, step = 9700 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:42,159 INFO (MainThread-139) global_step/sec: 112.365\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28312766551971436, step = 9800 (0.888 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:42,162 INFO (MainThread-139) loss = 0.28312766551971436, step = 9800 (0.888 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:43,050 INFO (MainThread-139) global_step/sec: 112.279\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.29897740483283997, step = 9900 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:43,053 INFO (MainThread-139) loss = 0.29897740483283997, step = 9900 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:43,939 INFO (MainThread-139) global_step/sec: 112.494\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26126745343208313, step = 10000 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:43,942 INFO (MainThread-139) loss = 0.26126745343208313, step = 10000 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:44,834 INFO (MainThread-139) global_step/sec: 111.707\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2524701654911041, step = 10100 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:44,837 INFO (MainThread-139) loss = 0.2524701654911041, step = 10100 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:45,728 INFO (MainThread-139) global_step/sec: 111.924\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2457617223262787, step = 10200 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:45,731 INFO (MainThread-139) loss = 0.2457617223262787, step = 10200 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:46,613 INFO (MainThread-139) global_step/sec: 112.96\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.28748181462287903, step = 10300 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:46,616 INFO (MainThread-139) loss = 0.28748181462287903, step = 10300 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:47,498 INFO (MainThread-139) global_step/sec: 113.005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26750627160072327, step = 10400 (0.885 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:47,501 INFO (MainThread-139) loss = 0.26750627160072327, step = 10400 (0.885 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:48,391 INFO (MainThread-139) global_step/sec: 111.942\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.16013191640377045, step = 10500 (0.894 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:48,394 INFO (MainThread-139) loss = 0.16013191640377045, step = 10500 (0.894 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:49,281 INFO (MainThread-139) global_step/sec: 112.414\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2715989053249359, step = 10600 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:49,283 INFO (MainThread-139) loss = 0.2715989053249359, step = 10600 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:50,179 INFO (MainThread-139) global_step/sec: 111.28\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.17849503457546234, step = 10700 (0.899 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:50,183 INFO (MainThread-139) loss = 0.17849503457546234, step = 10700 (0.899 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:51,078 INFO (MainThread-139) global_step/sec: 111.208\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1903795599937439, step = 10800 (0.899 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:51,081 INFO (MainThread-139) loss = 0.1903795599937439, step = 10800 (0.899 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:51,973 INFO (MainThread-139) global_step/sec: 111.762\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1765686422586441, step = 10900 (0.895 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:51,976 INFO (MainThread-139) loss = 0.1765686422586441, step = 10900 (0.895 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:52,853 INFO (MainThread-139) global_step/sec: 113.658\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18574108183383942, step = 11000 (0.879 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:52,856 INFO (MainThread-139) loss = 0.18574108183383942, step = 11000 (0.879 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:53,732 INFO (MainThread-139) global_step/sec: 113.734\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.16825605928897858, step = 11100 (0.882 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:53,737 INFO (MainThread-139) loss = 0.16825605928897858, step = 11100 (0.882 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:54,619 INFO (MainThread-139) global_step/sec: 112.786\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1854235827922821, step = 11200 (0.884 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:54,621 INFO (MainThread-139) loss = 0.1854235827922821, step = 11200 (0.884 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:55,506 INFO (MainThread-139) global_step/sec: 112.731\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.19780993461608887, step = 11300 (0.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:55,512 INFO (MainThread-139) loss = 0.19780993461608887, step = 11300 (0.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:56,396 INFO (MainThread-139) global_step/sec: 112.301\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.16918009519577026, step = 11400 (0.887 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:56,399 INFO (MainThread-139) loss = 0.16918009519577026, step = 11400 (0.887 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:57,285 INFO (MainThread-139) global_step/sec: 112.514\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.26178035140037537, step = 11500 (0.889 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:57,288 INFO (MainThread-139) loss = 0.26178035140037537, step = 11500 (0.889 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 113.043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:58,170 INFO (MainThread-139) global_step/sec: 113.043\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3194294571876526, step = 11600 (0.886 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:58,174 INFO (MainThread-139) loss = 0.3194294571876526, step = 11600 (0.886 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 111.388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:59,068 INFO (MainThread-139) global_step/sec: 111.388\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.22298800945281982, step = 11700 (0.897 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:59,071 INFO (MainThread-139) loss = 0.22298800945281982, step = 11700 (0.897 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 112.465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:59,957 INFO (MainThread-139) global_step/sec: 112.465\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.27747830748558044, step = 11800 (0.892 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:39:59,962 INFO (MainThread-139) loss = 0.27747830748558044, step = 11800 (0.892 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 11900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:40:00,847 INFO (MainThread-139) Saving checkpoints for 11900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.16469278931617737.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:40:00,934 INFO (MainThread-139) Loss for final step: 0.16469278931617737.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f12997d40b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBzcJU8Ajv2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_map(gt, pred, average=None):\n",
        "    \"\"\"\n",
        "    Compute the multi-label classification accuracy.\n",
        "    gt (np.ndarray): Shape Nx20, 0 or 1, 1 if the object i is present in that\n",
        "        image.\n",
        "    pred (np.ndarray): Shape Nx20, probability of that object in the image\n",
        "        (output probablitiy).\n",
        "    \n",
        "    \"\"\"\n",
        "    nclasses = gt.shape[1]\n",
        "    all_ap = []\n",
        "    for cid in range(nclasses):\n",
        "      gt_cls = gt[:, cid].astype('float32')\n",
        "      pred_cls = pred[:, cid].astype('float32')\n",
        "        # As per PhilK. code:\n",
        "        # https://github.com/philkr/voc-classification/blob/master/src/train_cls.py\n",
        "      pred_cls -= 1e-5 * gt_cls\n",
        "      ap = sklearn.metrics.average_precision_score(gt_cls, pred_cls, average=average)\n",
        "      all_ap.append(ap)\n",
        "    return all_ap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtpweU3HoVyT",
        "colab_type": "code",
        "outputId": "d25e5aa0-4b55-4ef3-8804-c89105f2b99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "map_array = np.zeros((1000,1), dtype='float')\n",
        "for given_iter in range(0,10):\n",
        "  classifier.train(input_fn=train_input_fn, steps=100)\n",
        "  #pascal_classifier.evaluate(input_fn=train_input_fn, steps=10)\n",
        "  pred = list(classifier.predict(input_fn=eval_input_fn))\n",
        "  pred = np.stack([p['probabilities'] for p in pred])\n",
        "  AP = compute_map(eval_labels.values, pred, average=None)\n",
        "  print('Obtained {} mAP'.format(np.mean(AP)))\n",
        "  map_array[given_iter] = np.mean(AP)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,157 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,298 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,468 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,473 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,569 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,579 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17400\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,635 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,643 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17400 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:43,849 INFO (MainThread-139) Saving checkpoints for 17400 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.13077056407928467, step = 17400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:44,019 INFO (MainThread-139) loss = 0.13077056407928467, step = 17400\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17500 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:44,998 INFO (MainThread-139) Saving checkpoints for 17500 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12668675184249878.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,086 INFO (MainThread-139) Loss for final step: 0.12668675184249878.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,109 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,213 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,307 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,315 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,357 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:45,363 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.05639173423790821 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,395 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,710 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,881 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,886 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,980 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:55,990 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:56,046 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:56,055 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17500 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:56,269 INFO (MainThread-139) Saving checkpoints for 17500 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.13234035670757294, step = 17500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:56,430 INFO (MainThread-139) loss = 0.13234035670757294, step = 17500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17600 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,418 INFO (MainThread-139) Saving checkpoints for 17600 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12357545644044876.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,511 INFO (MainThread-139) Loss for final step: 0.12357545644044876.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,535 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,635 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,732 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,741 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17600\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,783 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:28:57,790 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.056391731433057875 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:07,746 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:07,887 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,058 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,062 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,312 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,320 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17600\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,377 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,385 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17600 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,594 INFO (MainThread-139) Saving checkpoints for 17600 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.1971564143896103, step = 17600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:08,762 INFO (MainThread-139) loss = 0.1971564143896103, step = 17600\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17700 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:09,748 INFO (MainThread-139) Saving checkpoints for 17700 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.27354252338409424.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:09,834 INFO (MainThread-139) Loss for final step: 0.27354252338409424.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:09,857 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:09,958 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:10,052 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:10,060 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:10,102 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:10,109 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.05639173835938602 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,331 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,479 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,651 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,656 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,755 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,765 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,820 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:20,829 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17700 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:21,044 INFO (MainThread-139) Saving checkpoints for 17700 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.17646947503089905, step = 17700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:21,212 INFO (MainThread-139) loss = 0.17646947503089905, step = 17700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17800 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,191 INFO (MainThread-139) Saving checkpoints for 17800 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.13021939992904663.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,277 INFO (MainThread-139) Loss for final step: 0.13021939992904663.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,301 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,399 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,490 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,503 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17800\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,543 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:22,549 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.05639173516163772 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:32,703 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,018 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,194 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,197 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,290 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,301 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17800\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,355 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,363 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17800 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,577 INFO (MainThread-139) Saving checkpoints for 17800 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.17180217802524567, step = 17800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:33,740 INFO (MainThread-139) loss = 0.17180217802524567, step = 17800\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:34,723 INFO (MainThread-139) Saving checkpoints for 17900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.1349921077489853.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:34,810 INFO (MainThread-139) Loss for final step: 0.1349921077489853.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:34,834 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:34,934 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:35,028 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:35,039 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:35,081 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:35,088 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.056391735206282545 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,091 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,236 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,406 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,411 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,511 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-17900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,521 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-17900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,577 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,587 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 17900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,793 INFO (MainThread-139) Saving checkpoints for 17900 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.12659314274787903, step = 17900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:45,961 INFO (MainThread-139) loss = 0.12659314274787903, step = 17900\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18000 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:46,941 INFO (MainThread-139) Saving checkpoints for 18000 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.1253960281610489.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,030 INFO (MainThread-139) Loss for final step: 0.1253960281610489.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,052 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,155 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,418 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,428 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,473 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:47,478 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.056391731930168276 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:57,844 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:57,990 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,162 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,167 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,265 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,275 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,331 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,339 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18000 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,546 INFO (MainThread-139) Saving checkpoints for 18000 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.14148259162902832, step = 18000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:58,715 INFO (MainThread-139) loss = 0.14148259162902832, step = 18000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18100 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:59,693 INFO (MainThread-139) Saving checkpoints for 18100 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.18213944137096405.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:59,787 INFO (MainThread-139) Loss for final step: 0.18213944137096405.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:59,809 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:29:59,910 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:00,005 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:00,014 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:00,056 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:00,062 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.05639261901332159 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,144 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,288 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,622 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,626 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,720 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,729 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,785 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:10,793 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18100 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:11,013 INFO (MainThread-139) Saving checkpoints for 18100 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2633002996444702, step = 18100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:11,185 INFO (MainThread-139) loss = 0.2633002996444702, step = 18100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18200 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,169 INFO (MainThread-139) Saving checkpoints for 18200 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12602877616882324.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,266 INFO (MainThread-139) Loss for final step: 0.12602877616882324.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,290 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,393 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,487 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,496 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,537 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:12,545 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.056392762149137604 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:22,735 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:22,886 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,059 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,062 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,159 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,168 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,227 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,236 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18200 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,444 INFO (MainThread-139) Saving checkpoints for 18200 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.3220617175102234, step = 18200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:23,608 INFO (MainThread-139) loss = 0.3220617175102234, step = 18200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18300 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,587 INFO (MainThread-139) Saving checkpoints for 18300 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.26914116740226746.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,673 INFO (MainThread-139) Loss for final step: 0.26914116740226746.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,697 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,798 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,894 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,903 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18300\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,946 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:24,953 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.056392766617310304 mAP\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,180 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,325 INFO (MainThread-139) Summary name train loss is illegal; using train_loss instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,499 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,503 INFO (MainThread-139) Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,599 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,609 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18300\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,664 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,672 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18300 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:35,874 INFO (MainThread-139) Saving checkpoints for 18300 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.25482311844825745, step = 18300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:36,042 INFO (MainThread-139) loss = 0.25482311844825745, step = 18300\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 18400 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,017 INFO (MainThread-139) Saving checkpoints for 18400 into ./pascal2_models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12331423163414001.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,107 INFO (MainThread-139) Loss for final step: 0.12331423163414001.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,132 INFO (MainThread-139) Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,239 INFO (MainThread-139) Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,334 INFO (MainThread-139) Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./pascal2_models/model.ckpt-18400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,345 INFO (MainThread-139) Restoring parameters from ./pascal2_models/model.ckpt-18400\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,391 INFO (MainThread-139) Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 04:30:37,397 INFO (MainThread-139) Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained 0.05639276284927741 mAP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YhRbkaDY6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(targ, y_pred):\n",
        "  return np.sqrt(sklearn.metrics.mean_squared_error(np.exp(y_pred), np.exp(targ)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLtMEHqiqkVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -rf pyspark /usr/local/lib/python3.6/dist-packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYh7wo96IOTT",
        "colab_type": "code",
        "outputId": "f059d30e-0625-44e1-b577-648904763796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!virtualenv flask-aws"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: virtualenv: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}